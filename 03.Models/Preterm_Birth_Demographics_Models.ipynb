{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import dataframe with cleaned features and response variable from Data_Cleaning_Demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: import-ipynb in /home/jupyter/.local/lib/python3.7/site-packages (0.1.4)\n",
      "Requirement already satisfied: nbformat in /opt/conda/lib/python3.7/site-packages (from import-ipynb) (5.1.3)\n",
      "Requirement already satisfied: IPython in /opt/conda/lib/python3.7/site-packages (from import-ipynb) (7.32.0)\n",
      "Requirement already satisfied: setuptools>=18.5 in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (59.8.0)\n",
      "Requirement already satisfied: decorator in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (4.4.2)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (3.0.27)\n",
      "Requirement already satisfied: backcall in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: pickleshare in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (0.7.5)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (4.8.0)\n",
      "Requirement already satisfied: pygments in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (2.12.0)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (0.1.3)\n",
      "Requirement already satisfied: traitlets>=4.2 in /opt/conda/lib/python3.7/site-packages (from IPython->import-ipynb) (5.1.1)\n",
      "Requirement already satisfied: ipython-genutils in /opt/conda/lib/python3.7/site-packages (from nbformat->import-ipynb) (0.2.0)\n",
      "Requirement already satisfied: jupyter-core in /opt/conda/lib/python3.7/site-packages (from nbformat->import-ipynb) (4.9.2)\n",
      "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /opt/conda/lib/python3.7/site-packages (from nbformat->import-ipynb) (4.4.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/conda/lib/python3.7/site-packages (from jedi>=0.16->IPython->import-ipynb) (0.8.3)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (5.4.0)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (0.18.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (21.4.0)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (4.11.1)\n",
      "Requirement already satisfied: typing-extensions in /home/jupyter/.local/lib/python3.7/site-packages (from jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (4.7.1)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/conda/lib/python3.7/site-packages (from pexpect>4.3->IPython->import-ipynb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/conda/lib/python3.7/site-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->IPython->import-ipynb) (0.2.5)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /opt/conda/lib/python3.7/site-packages (from importlib-resources>=1.4.0->jsonschema!=2.5.0,>=2.4->nbformat->import-ipynb) (3.7.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thanks for your patience while I import the dataframe.\n",
      "All done.\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "print(\"Thanks for your patience while I import the dataframe.\")\n",
    "from Data_Cleaning_Demographics import *\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.metrics import precision_score\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Demographics Dataset from Data_Cleaning_Demographics_FINAL\n",
    "\n",
    "- final dataframe is 'birth'\n",
    "- response variable is 'birth_class_binary'\n",
    "- 'preprocessor' is pipeline for one-hot encoding race and ethnicity and scaling numerical features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test splits\n",
    "\n",
    "- Stratified splits\n",
    "The preterm birth rate in the United States is about 10%. Our data reflects this (although slightly higher), so we will be using stratified splits to ensure that our training and test sets both contain preterm births. \n",
    "  \n",
    "    \n",
    "- Multiple observations\n",
    "We are using multiple observations per person. To avoid using future data to predict the past, the *last* birth for each person will be reserved for the test set only. The training set may contain all births up to the last birth, including the first birth if it is the only birth. This is not a perfect solution to avoid data leakage, but the first thing we are trying. (*Note that this didn't end up being an issue here because of data quality.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark births that are the last birth of multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       person_id  birth_order  last_birth_of_multiples\n",
      "0        1000131            1                        0\n",
      "1        1000195            1                        0\n",
      "2        1000724            1                        0\n",
      "3        1001000            1                        0\n",
      "4        1001034            1                        0\n",
      "...          ...          ...                      ...\n",
      "13685    9981792            1                        0\n",
      "13686    9982785            1                        0\n",
      "13687    9989602            1                        0\n",
      "13688    9989602            2                        0\n",
      "13689    9989602            3                        1\n",
      "\n",
      "[13690 rows x 3 columns]\n",
      "Number of last births of multiples 2702\n",
      "Term       88.119911\n",
      "Preterm    11.880089\n",
      "Name: birth_class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'birth' DataFrame has columns 'person_id' and 'birth_order'\n",
    "\n",
    "# Sort the DataFrame by 'person_id' and 'birth_order'\n",
    "birth = birth.sort_values(by=['person_id', 'birth_order'])\n",
    "\n",
    "# Identify the last birth in each group (person_id) with more than one birth\n",
    "last_births = birth[birth.duplicated(subset='person_id', keep=False) & ~birth.duplicated(subset='person_id', keep='last')]\n",
    "\n",
    "# Create a new column 'last_birth_of_multiples' and mark the identified last births as 1\n",
    "birth['last_birth_of_multiples'] = 0\n",
    "birth.loc[last_births.index, 'last_birth_of_multiples'] = 1\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(birth[['person_id', 'birth_order', 'last_birth_of_multiples']])\n",
    "print(\"Number of last births of multiples\", birth['last_birth_of_multiples'].sum())\n",
    "\n",
    "# Check the distribution of term and preterm in this set \n",
    "filtered_mutliples_df = birth[birth['last_birth_of_multiples'] == 1]\n",
    "birth_class_percentage = filtered_mutliples_df['birth_class'].value_counts(normalize=True) * 100\n",
    "print(birth_class_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include marked last births only in the test set\n",
    "birth_test_data = birth[birth['last_birth_of_multiples'] == 1]\n",
    "\n",
    "# Exclude last births of multiples from the dataset\n",
    "birth_remaining = birth[birth['last_birth_of_multiples'] == 0]\n",
    "\n",
    "# Reset the index for the resulting DataFrames\n",
    "birth_test_data.reset_index(drop=True, inplace=True)\n",
    "birth_remaining.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Stratify the remaining births for inclusion in the training set\n",
    "birth_train_data, birth_test_remaining = train_test_split(birth_remaining,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True,\n",
    "                                                          random_state=404,\n",
    "                                                          stratify=birth_remaining['birth_class'])\n",
    "\n",
    "\n",
    "# Append last_birth_test_set to birth_test_remaining\n",
    "birth_test_data = birth_test_remaining.append(birth_test_data)\n",
    "\n",
    "\n",
    "# Reset the index for the resulting DataFrames\n",
    "birth_train_data.reset_index(drop=True, inplace=True)\n",
    "birth_test_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### Training and Test Data Set names:\n",
    "   Test Data Set: birth_test_data  \n",
    "   Training Data Set: birth_train_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View dataframes to check that multiples births loop worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Birth Train Data:\n",
      "birth train unique vals [1 2 3 4]\n",
      "\n",
      "Birth Test Data:\n",
      "birth test unique vals [1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "print(\"Birth Train Data:\")\n",
    "#print(birth_train_data.info())\n",
    "print(\"birth train unique vals\", birth_train_data['birth_order'].unique())\n",
    "\n",
    "print(\"\\nBirth Test Data:\")\n",
    "#print(birth_test_data.info())\n",
    "print(\"birth test unique vals\",birth_test_data['birth_order'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying train-test splits by inspecting percentage of preterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: counts of each birth class:\n",
      "Term       7516\n",
      "Preterm    1274\n",
      "Name: birth_class, dtype: int64\n",
      "Training Data: percentage of preterm births: 14.49%\n",
      "Test Data: counts of each birth class:\n",
      "Term       4261\n",
      "Preterm     639\n",
      "Name: birth_class, dtype: int64\n",
      "Test Data: percentage of preterm births: 13.04%\n"
     ]
    }
   ],
   "source": [
    "#Training Data\n",
    "birth_class_counts = birth_train_data['birth_class'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Training Data: counts of each birth class:\")\n",
    "print(birth_class_counts)\n",
    "\n",
    "# Calculate the percentage of preterm births\n",
    "preterm_percentage = (birth_class_counts['Preterm'] / birth_class_counts.sum()) * 100\n",
    "\n",
    "print(f\"Training Data: percentage of preterm births: {preterm_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "#Test Data\n",
    "birth_class_counts = birth_test_data['birth_class'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Test Data: counts of each birth class:\")\n",
    "print(birth_class_counts)\n",
    "\n",
    "# Calculate the percentage of preterm births\n",
    "preterm_percentage = (birth_class_counts['Preterm'] / birth_class_counts.sum()) * 100\n",
    "\n",
    "print(f\"Test Data: percentage of preterm births: {preterm_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized value counts:\n",
      "Term       0.855063\n",
      "Preterm    0.144937\n",
      "Name: birth_class, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Normalize train data\n",
    "normalized_value_counts = birth_train_data['birth_class'].value_counts(normalize=True)\n",
    "\n",
    "# Display the normalized value counts\n",
    "print(\"Normalized value counts:\")\n",
    "print(normalized_value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Based on our normalized value counts, our baseline model is a random coin flip with probability matching the likelihood of our preterm births (0.145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are some statistics for the recall score of our baseline:\n",
      "Mean Recall - 0.145\n",
      "Mean F1 Score - 0.145\n",
      "Mean PR AUC - 0.207\n",
      "Median Recall - 0.145\n",
      "Median F1 Score - 0.145\n",
      "Median PR AUC - 0.207\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Initialize empty lists to store evaluation metrics\n",
    "baseline_recalls = []\n",
    "baseline_f1_scores = []\n",
    "baseline_pr_aucs = []\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(404)\n",
    "\n",
    "# Perform 1000 random draws\n",
    "for obs in range(1000):\n",
    "    # Generate random binomial draws with probability 0.17\n",
    "    draw = np.random.binomial(n=1, p=0.145, size=len(birth_train_data))\n",
    "    \n",
    "    # Calculate recall score and append to the list\n",
    "    recall = recall_score(birth_train_data.birth_class_binary.values, draw)\n",
    "    baseline_recalls.append(recall)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_train_data.birth_class_binary.values, draw)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    baseline_pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Calculate F1 score and append to the list\n",
    "    f1 = f1_score(birth_train_data.birth_class_binary.values, draw)\n",
    "    baseline_f1_scores.append(f1)\n",
    "\n",
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are some statistics for the recall score of our baseline:\")\n",
    "print(\"Mean Recall - \" + str(round(np.mean(baseline_recalls), 3)))\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(baseline_f1_scores), 3)))\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(baseline_pr_aucs), 3)))\n",
    "print(\"Median Recall - \" + str(round(np.median(baseline_recalls), 3)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(baseline_f1_scores), 3)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(baseline_pr_aucs), 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Setting up for models\n",
    "\n",
    "We will use stratified 10-fold cross validation to account for the smaller percentage of pre-term births"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kfold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 10\n",
    "\n",
    "kfold = StratifiedKFold(kfold_splits, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['assisted_income_zip', 'high_school_education_zip', 'median_income_zip',\n",
    "              'no_health_insurance_zip', 'poverty_zip', 'vacant_housing_zip', 'deprivation_index_zip', 'age_at_birth']\n",
    "\n",
    "model_feat.extend([\"race_person\", \"ethnicity_person\"])\n",
    "model_feat.extend([\"birth_order\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost model - decision tree classifier with 50 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1117/806326747.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;31m# Fit the pipeline on training data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0madaboost_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbirth_tt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_feat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbirth_tt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbirth_class_binary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m      \u001b[0;31m# Get feature importances\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    392\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"passthrough\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;31m# Fit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    144\u001b[0m             \u001b[0;31m# Boosting step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             sample_weight, estimator_weight, estimator_error = self._boost(\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             )\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    546\u001b[0m         \"\"\"\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"SAMME.R\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# elif self.algorithm == \"SAMME\":\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_weight_boosting.py\u001b[0m in \u001b[0;36m_boost_real\u001b[0;34m(self, iboost, X, y, sample_weight, random_state)\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_boost_real\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miboost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;34m\"\"\"Implement a single boost using the SAMME.R real algorithm.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m         \u001b[0mestimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_estimator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_make_estimator\u001b[0;34m(self, append, random_state)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0m_set_random_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mappend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/ensemble/_base.py\u001b[0m in \u001b[0;36m_set_random_states\u001b[0;34m(estimator, random_state)\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mrandom_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_random_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mto_set\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"random_state\"\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__random_state\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mto_set\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36mget_params\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    207\u001b[0m         \"\"\"\n\u001b[1;32m    208\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_param_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"get_params\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/base.py\u001b[0m in \u001b[0;36m_get_param_names\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# introspect the constructor arguments to find the model parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[0;31m# to represent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0minit_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m         \u001b[0;31m# Consider the constructor parameters excluding 'self'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         parameters = [\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36msignature\u001b[0;34m(obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   3081\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3082\u001b[0m     \u001b[0;34m\"\"\"Get a signature object for the passed callable.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3083\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mSignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_wrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3084\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3085\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36mfrom_callable\u001b[0;34m(cls, obj, follow_wrapped)\u001b[0m\n\u001b[1;32m   2831\u001b[0m         \u001b[0;34m\"\"\"Constructs Signature for the given callable object.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2832\u001b[0m         return _signature_from_callable(obj, sigcls=cls,\n\u001b[0;32m-> 2833\u001b[0;31m                                         follow_wrapper_chains=follow_wrapped)\n\u001b[0m\u001b[1;32m   2834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2835\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_callable\u001b[0;34m(obj, follow_wrapper_chains, skip_bound_arg, sigcls)\u001b[0m\n\u001b[1;32m   2282\u001b[0m         \u001b[0;31m# If it's a pure Python function, or an object that is duck type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2283\u001b[0m         \u001b[0;31m# of a Python function (Cython functions, for instance), then:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2284\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_signature_from_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msigcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2286\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_signature_is_builtin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/inspect.py\u001b[0m in \u001b[0;36m_signature_from_function\u001b[0;34m(cls, func)\u001b[0m\n\u001b[1;32m   2150\u001b[0m     \u001b[0mnon_default_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpos_count\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpos_default_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpositional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnon_default_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2152\u001b[0;31m         \u001b[0mannotation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mannotations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_empty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2153\u001b[0m         parameters.append(Parameter(name, annotation=annotation,\n\u001b[1;32m   2154\u001b[0m                                     kind=_POSITIONAL_OR_KEYWORD))\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize arrays to store evaluation metrics\n",
    "ab_recalls = np.zeros(kfold_splits)\n",
    "ab_f1 = np.zeros(kfold_splits)\n",
    "ab_pr_auc = np.zeros(kfold_splits)\n",
    "\n",
    "# Initialize an empty dictionary to store feature importances\n",
    "feature_importance_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(birth_train_data, birth_train_data.birth_class_binary):\n",
    "    birth_tt = birth_train_data.iloc[train_index]\n",
    "    birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "    # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "    adaboost_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('adaboost', AdaBoostClassifier(n_estimators=50, random_state=404))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on training data\n",
    "    adaboost_pipeline.fit(birth_tt[model_feat], birth_tt.birth_class_binary)\n",
    "        \n",
    "     # Get feature importances\n",
    "    feature_importances = adaboost_pipeline.named_steps['adaboost'].feature_importances_\n",
    "\n",
    "    # Access the feature names after preprocessing\n",
    "    preprocessed_columns = adaboost_pipeline.named_steps['preprocessor'].get_feature_names_out(input_features=model_feat)\n",
    "\n",
    "    # Print feature names and their importances\n",
    "    for feature, importance in zip(preprocessed_columns, feature_importances):\n",
    "        #print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "        # Store feature importance in the dictionary\n",
    "        feature_importance_dict.setdefault(feature, []).append(importance)\n",
    "        \n",
    "    # Predict on the test data\n",
    "    ab_pred = adaboost_pipeline.predict(birth_ho[model_feat])\n",
    "    \n",
    "    # Calculate evaluation metrics using predictions on the test set\n",
    "    ab_recalls[counter] = recall_score(birth_ho.birth_class_binary, ab_pred)\n",
    "    ab_f1[counter] = f1_score(birth_ho.birth_class_binary, ab_pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    ab_precision, ab_recall, _ = precision_recall_curve(birth_ho.birth_class_binary, adaboost_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "    ab_pr_auc[counter] = auc(ab_recall, ab_precision)\n",
    "    \n",
    "    # Adjust counter for the next k-fold split\n",
    "    counter += 1\n",
    "    \n",
    "# Calculate mean feature importances across folds\n",
    "mean_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importance_dict.items()}\n",
    "\n",
    "# Print mean feature importances\n",
    "for feature, mean_importance in mean_feature_importances.items():\n",
    "    print(f\"Mean Feature Importance: {feature}, Mean Importance: {mean_importance.round(3)}\")\n",
    "\n",
    "print(\"Mean recall:\", np.mean(ab_recalls).round(3))\n",
    "print(\"Mean F1:\", np.mean(ab_f1).round(3))\n",
    "print(\"Mean PR-AUC\", np.mean(ab_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to keep\n",
    "\n",
    "We looked at feature importance from our AdaBoost model to inform feature selection for future models. The highest numbers are listed below. Ultimately, this did not improve model performance. \n",
    "\n",
    "**Mean Feature Importance: cat__race_person_Asian, Mean Importance: 0.006\n",
    "Mean Feature Importance: cat__race_person_Black or African American, Mean Importance: 0.033999999999999996\n",
    "Mean Feature Importance: cat__race_person_Middle Eastern or North African, Mean Importance: 0.019999999999999997\n",
    "Mean Feature Importance: cat__race_person_More than one population, Mean Importance: 0.019999999999999997\n",
    "Mean Feature Importance: cat__race_person_Native Hawaiian or Other Pacific Islander, Mean Importance: 0.002\n",
    "Mean Feature Importance: cat__race_person_None of these, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__race_person_White, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__race_person_no answer, Mean Importance: 0.0**\n",
    "\n",
    "Mean Feature Importance: cat__ethnicity_person_Hispanic or Latino, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__ethnicity_person_None of these, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__ethnicity_person_Not Hispanic or Latino, Mean Importance: 0.027999999999999997\n",
    "Mean Feature Importance: cat__ethnicity_person_no answer, Mean Importance: 0.014000000000000002\n",
    "\n",
    "Mean Feature Importance: cat__birth_order_1, Mean Importance: 0.008\n",
    "Mean Feature Importance: cat__birth_order_2, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__birth_order_3, Mean Importance: 0.014000000000000002\n",
    "Mean Feature Importance: cat__birth_order_4, Mean Importance: 0.0\n",
    "\n",
    "Mean Feature Importance: num__assisted_income_zip, Mean Importance: 0.054\n",
    "**Mean Feature Importance: num__high_school_education_zip, Mean Importance: 0.120**\n",
    "Mean Feature Importance: num__median_income_zip, Mean Importance: 0.062\n",
    "**Mean Feature Importance: num__no_health_insurance_zip, Mean Importance: 0.138**\n",
    "Mean Feature Importance: num__poverty_zip, Mean Importance: 0.102\n",
    "**Mean Feature Importance: num__vacant_housing_zip, Mean Importance: 0.144**\n",
    "Mean Feature Importance: num__deprivation_index_zip, Mean Importance: 0.060\n",
    "**Mean Feature Importance: num__age_at_birth, Mean Importance: 0.174**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update model features based on feature importance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_feat = ['high_school_education_zip', 'no_health_insurance_zip', 'vacant_housing_zip', 'age_at_birth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update preprocessor to transform a smaller number of columns\n",
    "\n",
    "Updated pipeline for using a subset of features based on feature selection. Commented out as the subset of features performed worse than the full set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_columns = ['high_school_education_zip', 'no_health_insurance_zip', 'vacant_housing_zip', 'age_at_birth']\n",
    "\n",
    "\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#          ('num', StandardScaler(), numeric_columns)\n",
    "#    ],\n",
    "#    remainder='passthrough'  # Keeps the non-categorical columns as they are\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost with Log Regression, 50 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean recall: 0.0\n",
      "Mean F1: 0.0\n",
      "Mean PR-AUC: 0.179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize arrays to store evaluation metrics\n",
    "ab_recalls = np.zeros(kfold_splits)\n",
    "ab_f1 = np.zeros(kfold_splits)\n",
    "ab_pr_auc = np.zeros(kfold_splits)\n",
    "counter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(birth_train_data, birth_train_data['birth_class_binary']):\n",
    "    birth_tt = birth_train_data.iloc[train_index]\n",
    "    birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "    # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "    adaboost_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('adaboost', AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=404))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on training data\n",
    "    adaboost_pipeline.fit(birth_tt[model_feat], birth_tt['birth_class_binary'])\n",
    "\n",
    "    # Predict on the holdout data\n",
    "    ab_pred = adaboost_pipeline.predict(birth_ho[model_feat])\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    ab_recalls[counter] = recall_score(birth_ho['birth_class_binary'], ab_pred)\n",
    "    ab_f1[counter] = f1_score(birth_ho['birth_class_binary'], ab_pred)\n",
    "\n",
    "    # Calculate precision-recall curve and AUC\n",
    "    ab_precision, ab_recall, _ = precision_recall_curve(birth_ho['birth_class_binary'], adaboost_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "    ab_pr_auc[counter] = auc(ab_recall, ab_precision)\n",
    "\n",
    "    # Adjust counter for the next k-fold split\n",
    "    counter += 1\n",
    "\n",
    "print(\"Mean recall:\", np.mean(ab_recalls).round(3))\n",
    "print(\"Mean F1:\", np.mean(ab_f1).round(3))\n",
    "print(\"Mean PR-AUC:\", np.mean(ab_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC model with class weights and loop to print values for multiple class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "Now fitting the model. Please be patient.\n",
      "weight: 5.75\n",
      "Mean recall 0.458\n",
      "Mean F1 0.257\n",
      "Mean PR-AUC 0.178\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "weights = [5.75] #add numbers to explore more weights\n",
    "\n",
    "for weight in weights:\n",
    "    \n",
    "    # Initialize arrays to store evaluation metrics\n",
    "    svc_recalls = np.zeros(kfold_splits)\n",
    "    svc_f1 = np.zeros(kfold_splits)\n",
    "    svc_pr_auc = np.zeros(kfold_splits)\n",
    "\n",
    "    # Initialize list to store predictions\n",
    "    svc_predictions = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for train_index, test_index in kfold.split(birth_train_data, birth_train_data.birth_class_binary):\n",
    "        birth_tt = birth_train_data.iloc[train_index]\n",
    "        birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "        # Assuming class_weights is a dictionary containing class weights; preterm birth (1) should have heavier weight\n",
    "        class_weights = {0: 1, 1: weight}  \n",
    "    \n",
    "        # Add a column for class weights to birth_tt\n",
    "        birth_tt = birth_tt.copy()\n",
    "        birth_tt['class_weights'] = birth_tt['birth_class_binary'].map(class_weights)\n",
    "    \n",
    "        # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "        svc_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('svc', SVC(probability=True, random_state=404))\n",
    "        ])\n",
    "\n",
    "        print(\"Now fitting the model. Please be patient.\")\n",
    "        # Fit the pipeline on training data\n",
    "        svc_pipeline.fit(birth_tt[model_feat], birth_tt['birth_class_binary'], svc__sample_weight=birth_tt['class_weights'])\n",
    "\n",
    "        # Predict on the holdout data\n",
    "        svc_pred = svc_pipeline.predict(birth_ho[model_feat])\n",
    "\n",
    "        # Append predictions to the list\n",
    "        svc_predictions.append(svc_pred)\n",
    "    \n",
    "        # Calculate evaluation metrics\n",
    "        svc_recalls[counter] = recall_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "        #print(\"fold\", counter, \"recall\", svc_recalls[counter])    \n",
    "        svc_f1[counter] = f1_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "        #print(\"fold\", counter, \"f1\", svc_f1[counter]) \n",
    "    \n",
    "        # Calculate precision-recall curve and AUC\n",
    "        svc_precision, svc_recall, _ = precision_recall_curve(birth_ho.birth_class_binary, svc_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "        svc_pr_auc[counter] = auc(svc_recall, svc_precision)\n",
    "        #print(\"fold\", counter, \"pr-auc\", svc_pr_auc[counter]) \n",
    "    \n",
    "        # Adjust counter for the next k-fold split\n",
    "        counter += 1\n",
    "        #print(\"onto the next fold\") \n",
    "\n",
    "\n",
    "#print(\"Recalls\", svc_recalls)\n",
    "#print(\"F1 Scores\", svc_f1)\n",
    "#print(\"PR-AUC\", svc_pr_auc)\n",
    "    print(\"weight:\", weight)\n",
    "    print(\"Mean recall\", np.mean(svc_recalls).round(3))\n",
    "    print(\"Mean F1\", np.mean(svc_f1).round(3))\n",
    "    print(\"Mean PR-AUC\", np.mean(svc_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I tried to maximize the PR-AUC, which is still performing below baseline. I'm not able to improve this model further with class weights alone. Basically the model is just over-predicting preterm births. These are some scores for \n",
    "\n",
    "model_feat = ['assisted_income_zip', 'high_school_education_zip', 'median_income_zip',\n",
    "              'no_health_insurance_zip', 'poverty_zip', 'vacant_housing_zip', 'deprivation_index_zip', 'age_at_birth']  \n",
    "              \n",
    "model_feat.extend([\"race_person\", \"ethnicity_person\"])  \n",
    "\n",
    "model_feat.extend([\"birth_order\"])  \n",
    "\n",
    "weight: 5  \n",
    "Mean recall 0.291\n",
    "Mean F1 0.237 \n",
    "Mean PR-AUC 0.182\n",
    "\n",
    "weight: 5.25  \n",
    "Mean recall 0.320\n",
    "Mean F1 0.241\n",
    "Mean PR-AUC 0.180  \n",
    "\n",
    "weight: 5.5  \n",
    "Mean recall 0.355 \n",
    "Mean F1 0.244\n",
    "Mean PR-AUC 0.179  \n",
    "\n",
    "**weight: 5.75  \n",
    "Mean recall 0.434  \n",
    "Mean F1 0.261  \n",
    "Mean PR-AUC 0.181**  \n",
    "\n",
    "weight: 6  \n",
    "Mean recall 0.464  \n",
    "Mean F1 0.250  \n",
    "Mean PR-AUC 0.179  \n",
    "\n",
    "weight: 6.25  \n",
    "Mean recall 0.521  \n",
    "Mean F1 0.253  \n",
    "Mean PR-AUC 0.177  \n",
    "\n",
    "weight: 6.5  \n",
    "Mean recall 0.581  \n",
    "Mean F1 0.255  \n",
    "Mean PR-AUC 0.174  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[499 252]\n",
      " [ 74  54]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.66      0.75       751\n",
      "           1       0.18      0.42      0.25       128\n",
      "\n",
      "    accuracy                           0.63       879\n",
      "   macro avg       0.52      0.54      0.50       879\n",
      "weighted avg       0.77      0.63      0.68       879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(birth_ho['birth_class_binary'], svc_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(birth_ho['birth_class_binary'], svc_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with GridSearch CV\n",
    "\n",
    "Remembered there is a better way to tune hyperparameters than the loop above....\n",
    "\n",
    "Checked with:\n",
    "param_grid = {\n",
    "     'svc__C': [.1, 1, 10],  \n",
    "    'svc__kernel': ['linear', 'poly', 'rbf' ],\n",
    "    'svc__class_weight': [{0: 1, 1: weight} for weight in np.arange(5, 7, 0.25)]\n",
    "}\n",
    "\n",
    "param_grid below updated to best model in order to save computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created the GridSearch SV object. Now fitting the model. Please be patient.\n",
      "Best Hyperparameters: {'svc__C': 1, 'svc__class_weight': {0: 1, 1: 5.75}, 'svc__kernel': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "     'svc__C': [1],  \n",
    "    'svc__kernel': ['linear'],\n",
    "    'svc__class_weight': [{0: 1, 1: 5.75}]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create pipeline using preprocessor and initial hyperparameter settings (must match hyperparameters in param grid)\n",
    "gridsearch_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc', SVC(C=1.0, kernel='linear', class_weight=None, probability=True, random_state=404)) \n",
    "])\n",
    "\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gridsearch_pipeline, param_grid=param_grid, scoring='average_precision', cv=5)  \n",
    "print(\"Created the GridSearch SV object. Now fitting the model. Please be patient.\")\n",
    "\n",
    "# Fit the GridSearchCV object to your data\n",
    "grid_search.fit(birth_tt[model_feat], birth_tt['birth_class_binary'])\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting best hyperparameters for SVC model (class weights, kernel, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Kernel: linear\n",
      "Best C: 1\n",
      "Best Class Weight: {0: 1, 1: 5.75}\n",
      "PR AUC of the Best Model: 0.182\n"
     ]
    }
   ],
   "source": [
    "# Accessing hyperparameters\n",
    "best_kernel = best_model.named_steps['svc'].kernel\n",
    "best_C = best_model.named_steps['svc'].C\n",
    "best_class_weight = best_model.named_steps['svc'].class_weight\n",
    "\n",
    "# Print or use the values as needed\n",
    "print(\"Best Kernel:\", best_kernel)\n",
    "print(\"Best C:\", best_C)\n",
    "print(\"Best Class Weight:\", best_class_weight)\n",
    "\n",
    "\n",
    "#Make predictions on the test set using the best model\n",
    "gridsearch_pred = best_model.predict(birth_ho[model_feat])\n",
    "\n",
    "gridsearch_scores = best_model.decision_function(birth_ho[model_feat])\n",
    "\n",
    "\n",
    "# Print scores\n",
    "pr_auc = average_precision_score(birth_ho['birth_class_binary'], gridsearch_scores)\n",
    "print(\"PR AUC of the Best Model:\", pr_auc.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[603 148]\n",
      " [ 87  41]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.80      0.84       751\n",
      "           1       0.22      0.32      0.26       128\n",
      "\n",
      "    accuracy                           0.73       879\n",
      "   macro avg       0.55      0.56      0.55       879\n",
      "weighted avg       0.78      0.73      0.75       879\n",
      "\n"
     ]
    }
   ],
   "source": [
    "###### Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(birth_ho['birth_class_binary'], gridsearch_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(birth_ho['birth_class_binary'], gridsearch_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: SVC model with final hyperparameters and pipeline\n",
    "\n",
    "- Chose 6.25; highest PR-AUC achieved  \n",
    "\n",
    "Recalls 0.5492957746478874  \n",
    "F1 Scores 0.23330009970089732  \n",
    "PR-AUC 0.15802294790524057  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recalls 0.433\n",
      "F1 Scores 0.224\n",
      "PR-AUC 0.158\n"
     ]
    }
   ],
   "source": [
    "#Add class weights \n",
    "class_weights = {0: 1, 1: 5.75}  \n",
    "birth_test_data = birth_test_data.copy()\n",
    "birth_test_data['class_weights'] = birth_test_data['birth_class_binary'].map(class_weights)\n",
    "\n",
    "# Predict on the test data\n",
    "svc_pred_final = svc_pipeline.predict(birth_test_data[model_feat])\n",
    "        \n",
    "# Calculate evaluation metrics\n",
    "svc_recalls = recall_score(birth_test_data.birth_class_binary.values, svc_pred_final)   \n",
    "svc_f1 = f1_score(birth_test_data.birth_class_binary.values, svc_pred_final)\n",
    "    \n",
    "# Calculate precision-recall curve and AUC\n",
    "svc_precision, svc_recall, _ = precision_recall_curve(birth_test_data.birth_class_binary, svc_pipeline.predict_proba(birth_test_data[model_feat])[:, 1])\n",
    "svc_pr_auc = auc(svc_recall, svc_precision)\n",
    "    \n",
    "\n",
    "print(\"Recalls\", svc_recalls.round(3))\n",
    "print(\"F1 Scores\", svc_f1.round(3))\n",
    "print(\"PR-AUC\", svc_pr_auc.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for fairness metrics because I couldn't get the pipeline to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_encoded - entire birth dataframe for metrics on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>assisted_income_zip</th>\n",
       "      <th>high_school_education_zip</th>\n",
       "      <th>median_income_zip</th>\n",
       "      <th>no_health_insurance_zip</th>\n",
       "      <th>poverty_zip</th>\n",
       "      <th>vacant_housing_zip</th>\n",
       "      <th>deprivation_index_zip</th>\n",
       "      <th>BMI</th>\n",
       "      <th>gestational_age_at_birth</th>\n",
       "      <th>birth_order</th>\n",
       "      <th>birth_class_binary</th>\n",
       "      <th>age_at_birth</th>\n",
       "      <th>last_birth_of_multiples</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black or African American</th>\n",
       "      <th>race_Middle Eastern or North African</th>\n",
       "      <th>race_More than one population</th>\n",
       "      <th>race_Native Hawaiian or Other Pacific Islander</th>\n",
       "      <th>race_None of these</th>\n",
       "      <th>race_White</th>\n",
       "      <th>race_no answer</th>\n",
       "      <th>ethnicity_Hispanic or Latino</th>\n",
       "      <th>ethnicity_None of these</th>\n",
       "      <th>ethnicity_Not Hispanic or Latino</th>\n",
       "      <th>ethnicity_no answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000131</td>\n",
       "      <td>13.019775</td>\n",
       "      <td>85.134547</td>\n",
       "      <td>61580.823283</td>\n",
       "      <td>10.762228</td>\n",
       "      <td>13.981854</td>\n",
       "      <td>12.360850</td>\n",
       "      <td>0.331276</td>\n",
       "      <td>36.500000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000195</td>\n",
       "      <td>38.279736</td>\n",
       "      <td>70.896923</td>\n",
       "      <td>39407.415829</td>\n",
       "      <td>11.049983</td>\n",
       "      <td>29.653439</td>\n",
       "      <td>5.505385</td>\n",
       "      <td>0.485979</td>\n",
       "      <td>28.299999</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1000724</td>\n",
       "      <td>20.910346</td>\n",
       "      <td>88.333925</td>\n",
       "      <td>42632.000716</td>\n",
       "      <td>6.287592</td>\n",
       "      <td>18.639903</td>\n",
       "      <td>15.527784</td>\n",
       "      <td>0.360499</td>\n",
       "      <td>40.599998</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1001000</td>\n",
       "      <td>15.587510</td>\n",
       "      <td>66.084673</td>\n",
       "      <td>69333.203649</td>\n",
       "      <td>16.807387</td>\n",
       "      <td>16.542301</td>\n",
       "      <td>2.669649</td>\n",
       "      <td>0.397667</td>\n",
       "      <td>28.700001</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001034</td>\n",
       "      <td>18.509215</td>\n",
       "      <td>78.387333</td>\n",
       "      <td>54223.044605</td>\n",
       "      <td>17.148201</td>\n",
       "      <td>22.064159</td>\n",
       "      <td>10.368943</td>\n",
       "      <td>0.409916</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id  assisted_income_zip  high_school_education_zip  \\\n",
       "0    1000131            13.019775                  85.134547   \n",
       "1    1000195            38.279736                  70.896923   \n",
       "2    1000724            20.910346                  88.333925   \n",
       "3    1001000            15.587510                  66.084673   \n",
       "4    1001034            18.509215                  78.387333   \n",
       "\n",
       "   median_income_zip  no_health_insurance_zip  poverty_zip  \\\n",
       "0       61580.823283                10.762228    13.981854   \n",
       "1       39407.415829                11.049983    29.653439   \n",
       "2       42632.000716                 6.287592    18.639903   \n",
       "3       69333.203649                16.807387    16.542301   \n",
       "4       54223.044605                17.148201    22.064159   \n",
       "\n",
       "   vacant_housing_zip  deprivation_index_zip        BMI  \\\n",
       "0           12.360850               0.331276  36.500000   \n",
       "1            5.505385               0.485979  28.299999   \n",
       "2           15.527784               0.360499  40.599998   \n",
       "3            2.669649               0.397667  28.700001   \n",
       "4           10.368943               0.409916  28.900000   \n",
       "\n",
       "   gestational_age_at_birth  birth_order  birth_class_binary  age_at_birth  \\\n",
       "0                      39.0            1                   0          19.0   \n",
       "1                      40.0            1                   0          39.0   \n",
       "2                      40.0            1                   0          20.0   \n",
       "3                      40.0            1                   0          19.0   \n",
       "4                      39.0            1                   0          28.0   \n",
       "\n",
       "   last_birth_of_multiples  race_Asian  race_Black or African American  \\\n",
       "0                        0           0                               0   \n",
       "1                        0           0                               1   \n",
       "2                        0           0                               0   \n",
       "3                        0           0                               1   \n",
       "4                        0           0                               0   \n",
       "\n",
       "   race_Middle Eastern or North African  race_More than one population  \\\n",
       "0                                     0                              0   \n",
       "1                                     0                              0   \n",
       "2                                     0                              0   \n",
       "3                                     0                              0   \n",
       "4                                     0                              0   \n",
       "\n",
       "   race_Native Hawaiian or Other Pacific Islander  race_None of these  \\\n",
       "0                                               0                   0   \n",
       "1                                               0                   0   \n",
       "2                                               0                   0   \n",
       "3                                               0                   0   \n",
       "4                                               0                   1   \n",
       "\n",
       "   race_White  race_no answer  ethnicity_Hispanic or Latino  \\\n",
       "0           1               0                             0   \n",
       "1           0               0                             0   \n",
       "2           1               0                             0   \n",
       "3           0               0                             1   \n",
       "4           0               0                             0   \n",
       "\n",
       "   ethnicity_None of these  ethnicity_Not Hispanic or Latino  \\\n",
       "0                        0                                 1   \n",
       "1                        0                                 1   \n",
       "2                        0                                 1   \n",
       "3                        0                                 0   \n",
       "4                        1                                 0   \n",
       "\n",
       "   ethnicity_no answer  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_encoded = pd.get_dummies(birth, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "# Drop datetime and obj col as it causes errors and is not necessary\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_encoded = birth_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "birth_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_ho encoding (birth holdout set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>assisted_income_zip</th>\n",
       "      <th>high_school_education_zip</th>\n",
       "      <th>median_income_zip</th>\n",
       "      <th>no_health_insurance_zip</th>\n",
       "      <th>poverty_zip</th>\n",
       "      <th>vacant_housing_zip</th>\n",
       "      <th>deprivation_index_zip</th>\n",
       "      <th>BMI</th>\n",
       "      <th>gestational_age_at_birth</th>\n",
       "      <th>birth_order</th>\n",
       "      <th>birth_class_binary</th>\n",
       "      <th>age_at_birth</th>\n",
       "      <th>last_birth_of_multiples</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black or African American</th>\n",
       "      <th>race_Middle Eastern or North African</th>\n",
       "      <th>race_More than one population</th>\n",
       "      <th>race_Native Hawaiian or Other Pacific Islander</th>\n",
       "      <th>race_None of these</th>\n",
       "      <th>race_White</th>\n",
       "      <th>race_no answer</th>\n",
       "      <th>ethnicity_Hispanic or Latino</th>\n",
       "      <th>ethnicity_None of these</th>\n",
       "      <th>ethnicity_Not Hispanic or Latino</th>\n",
       "      <th>ethnicity_no answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1855389</td>\n",
       "      <td>13.313458</td>\n",
       "      <td>93.789391</td>\n",
       "      <td>60080.505422</td>\n",
       "      <td>5.502843</td>\n",
       "      <td>14.925582</td>\n",
       "      <td>10.289364</td>\n",
       "      <td>0.291886</td>\n",
       "      <td>22.799999</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3360549</td>\n",
       "      <td>17.355981</td>\n",
       "      <td>85.665558</td>\n",
       "      <td>83951.707638</td>\n",
       "      <td>6.863952</td>\n",
       "      <td>17.753432</td>\n",
       "      <td>11.598508</td>\n",
       "      <td>0.314289</td>\n",
       "      <td>34.900002</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3329907</td>\n",
       "      <td>11.564450</td>\n",
       "      <td>91.185311</td>\n",
       "      <td>54646.737513</td>\n",
       "      <td>7.238072</td>\n",
       "      <td>11.671013</td>\n",
       "      <td>10.933020</td>\n",
       "      <td>0.298404</td>\n",
       "      <td>27.700001</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>3456062</td>\n",
       "      <td>8.441550</td>\n",
       "      <td>86.816513</td>\n",
       "      <td>78108.845020</td>\n",
       "      <td>10.278120</td>\n",
       "      <td>14.373649</td>\n",
       "      <td>6.069094</td>\n",
       "      <td>0.291387</td>\n",
       "      <td>26.400000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>3180426</td>\n",
       "      <td>16.417796</td>\n",
       "      <td>86.924171</td>\n",
       "      <td>50953.900937</td>\n",
       "      <td>11.536400</td>\n",
       "      <td>19.940187</td>\n",
       "      <td>10.839851</td>\n",
       "      <td>0.360788</td>\n",
       "      <td>26.100000</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    person_id  assisted_income_zip  high_school_education_zip  \\\n",
       "1     1855389            13.313458                  93.789391   \n",
       "6     3360549            17.355981                  85.665558   \n",
       "7     3329907            11.564450                  91.185311   \n",
       "31    3456062             8.441550                  86.816513   \n",
       "41    3180426            16.417796                  86.924171   \n",
       "\n",
       "    median_income_zip  no_health_insurance_zip  poverty_zip  \\\n",
       "1        60080.505422                 5.502843    14.925582   \n",
       "6        83951.707638                 6.863952    17.753432   \n",
       "7        54646.737513                 7.238072    11.671013   \n",
       "31       78108.845020                10.278120    14.373649   \n",
       "41       50953.900937                11.536400    19.940187   \n",
       "\n",
       "    vacant_housing_zip  deprivation_index_zip        BMI  \\\n",
       "1            10.289364               0.291886  22.799999   \n",
       "6            11.598508               0.314289  34.900002   \n",
       "7            10.933020               0.298404  27.700001   \n",
       "31            6.069094               0.291387  26.400000   \n",
       "41           10.839851               0.360788  26.100000   \n",
       "\n",
       "    gestational_age_at_birth  birth_order  birth_class_binary  age_at_birth  \\\n",
       "1                       40.0            1                   0          27.0   \n",
       "6                       36.0            1                   1          39.0   \n",
       "7                       35.0            1                   1          38.0   \n",
       "31                      40.0            1                   0          30.0   \n",
       "41                      38.0            1                   0          38.0   \n",
       "\n",
       "    last_birth_of_multiples  race_Asian  race_Black or African American  \\\n",
       "1                         0           0                               0   \n",
       "6                         0           0                               1   \n",
       "7                         0           0                               0   \n",
       "31                        0           0                               0   \n",
       "41                        0           0                               0   \n",
       "\n",
       "    race_Middle Eastern or North African  race_More than one population  \\\n",
       "1                                      0                              0   \n",
       "6                                      0                              0   \n",
       "7                                      0                              0   \n",
       "31                                     0                              0   \n",
       "41                                     0                              0   \n",
       "\n",
       "    race_Native Hawaiian or Other Pacific Islander  race_None of these  \\\n",
       "1                                                0                   0   \n",
       "6                                                0                   0   \n",
       "7                                                0                   0   \n",
       "31                                               0                   0   \n",
       "41                                               0                   0   \n",
       "\n",
       "    race_White  race_no answer  ethnicity_Hispanic or Latino  \\\n",
       "1            1               0                             0   \n",
       "6            0               0                             1   \n",
       "7            1               0                             0   \n",
       "31           1               0                             0   \n",
       "41           1               0                             1   \n",
       "\n",
       "    ethnicity_None of these  ethnicity_Not Hispanic or Latino  \\\n",
       "1                         0                                 1   \n",
       "6                         0                                 0   \n",
       "7                         0                                 1   \n",
       "31                        0                                 1   \n",
       "41                        0                                 0   \n",
       "\n",
       "    ethnicity_no answer  \n",
       "1                     0  \n",
       "6                     0  \n",
       "7                     0  \n",
       "31                    0  \n",
       "41                    0  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_ho_encoded = pd.get_dummies(birth_ho, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_ho_encoded = birth_ho_encoded.drop(columns=columns_to_drop)\n",
    "birth_ho_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_test_data (Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>person_id</th>\n",
       "      <th>assisted_income_zip</th>\n",
       "      <th>high_school_education_zip</th>\n",
       "      <th>median_income_zip</th>\n",
       "      <th>no_health_insurance_zip</th>\n",
       "      <th>poverty_zip</th>\n",
       "      <th>vacant_housing_zip</th>\n",
       "      <th>deprivation_index_zip</th>\n",
       "      <th>BMI</th>\n",
       "      <th>gestational_age_at_birth</th>\n",
       "      <th>birth_order</th>\n",
       "      <th>birth_class_binary</th>\n",
       "      <th>age_at_birth</th>\n",
       "      <th>last_birth_of_multiples</th>\n",
       "      <th>class_weights</th>\n",
       "      <th>race_Asian</th>\n",
       "      <th>race_Black or African American</th>\n",
       "      <th>race_Middle Eastern or North African</th>\n",
       "      <th>race_More than one population</th>\n",
       "      <th>race_Native Hawaiian or Other Pacific Islander</th>\n",
       "      <th>race_None of these</th>\n",
       "      <th>race_White</th>\n",
       "      <th>race_no answer</th>\n",
       "      <th>ethnicity_Hispanic or Latino</th>\n",
       "      <th>ethnicity_None of these</th>\n",
       "      <th>ethnicity_Not Hispanic or Latino</th>\n",
       "      <th>ethnicity_no answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9258038</td>\n",
       "      <td>18.509215</td>\n",
       "      <td>78.387333</td>\n",
       "      <td>54223.044605</td>\n",
       "      <td>17.148201</td>\n",
       "      <td>22.064159</td>\n",
       "      <td>10.368943</td>\n",
       "      <td>0.409916</td>\n",
       "      <td>28.900000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3188400</td>\n",
       "      <td>16.192630</td>\n",
       "      <td>87.318666</td>\n",
       "      <td>71783.359081</td>\n",
       "      <td>3.705177</td>\n",
       "      <td>15.740995</td>\n",
       "      <td>6.615493</td>\n",
       "      <td>0.296992</td>\n",
       "      <td>19.799999</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>37.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1630649</td>\n",
       "      <td>17.355981</td>\n",
       "      <td>85.665558</td>\n",
       "      <td>83951.707638</td>\n",
       "      <td>6.863952</td>\n",
       "      <td>17.753432</td>\n",
       "      <td>11.598508</td>\n",
       "      <td>0.314289</td>\n",
       "      <td>45.700001</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1929111</td>\n",
       "      <td>38.279736</td>\n",
       "      <td>70.896923</td>\n",
       "      <td>39407.415829</td>\n",
       "      <td>11.049983</td>\n",
       "      <td>29.653439</td>\n",
       "      <td>5.505385</td>\n",
       "      <td>0.485979</td>\n",
       "      <td>24.600000</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2786018</td>\n",
       "      <td>17.057142</td>\n",
       "      <td>83.889684</td>\n",
       "      <td>56263.624672</td>\n",
       "      <td>9.410737</td>\n",
       "      <td>20.408753</td>\n",
       "      <td>6.198068</td>\n",
       "      <td>0.354028</td>\n",
       "      <td>36.400002</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   person_id  assisted_income_zip  high_school_education_zip  \\\n",
       "0    9258038            18.509215                  78.387333   \n",
       "1    3188400            16.192630                  87.318666   \n",
       "2    1630649            17.355981                  85.665558   \n",
       "3    1929111            38.279736                  70.896923   \n",
       "4    2786018            17.057142                  83.889684   \n",
       "\n",
       "   median_income_zip  no_health_insurance_zip  poverty_zip  \\\n",
       "0       54223.044605                17.148201    22.064159   \n",
       "1       71783.359081                 3.705177    15.740995   \n",
       "2       83951.707638                 6.863952    17.753432   \n",
       "3       39407.415829                11.049983    29.653439   \n",
       "4       56263.624672                 9.410737    20.408753   \n",
       "\n",
       "   vacant_housing_zip  deprivation_index_zip        BMI  \\\n",
       "0           10.368943               0.409916  28.900000   \n",
       "1            6.615493               0.296992  19.799999   \n",
       "2           11.598508               0.314289  45.700001   \n",
       "3            5.505385               0.485979  24.600000   \n",
       "4            6.198068               0.354028  36.400002   \n",
       "\n",
       "   gestational_age_at_birth  birth_order  birth_class_binary  age_at_birth  \\\n",
       "0                      40.0            1                   0          19.0   \n",
       "1                      39.0            1                   0          37.0   \n",
       "2                      38.0            1                   0          33.0   \n",
       "3                      40.0            1                   0          25.0   \n",
       "4                      37.0            1                   0          23.0   \n",
       "\n",
       "   last_birth_of_multiples  class_weights  race_Asian  \\\n",
       "0                        0            1.0           0   \n",
       "1                        0            1.0           0   \n",
       "2                        0            1.0           0   \n",
       "3                        0            1.0           0   \n",
       "4                        0            1.0           0   \n",
       "\n",
       "   race_Black or African American  race_Middle Eastern or North African  \\\n",
       "0                               1                                     0   \n",
       "1                               0                                     0   \n",
       "2                               1                                     0   \n",
       "3                               0                                     0   \n",
       "4                               0                                     0   \n",
       "\n",
       "   race_More than one population  \\\n",
       "0                              0   \n",
       "1                              0   \n",
       "2                              0   \n",
       "3                              0   \n",
       "4                              0   \n",
       "\n",
       "   race_Native Hawaiian or Other Pacific Islander  race_None of these  \\\n",
       "0                                               0                   0   \n",
       "1                                               0                   0   \n",
       "2                                               0                   0   \n",
       "3                                               0                   0   \n",
       "4                                               0                   0   \n",
       "\n",
       "   race_White  race_no answer  ethnicity_Hispanic or Latino  \\\n",
       "0           0               0                             0   \n",
       "1           0               1                             1   \n",
       "2           0               0                             0   \n",
       "3           0               1                             1   \n",
       "4           1               0                             0   \n",
       "\n",
       "   ethnicity_None of these  ethnicity_Not Hispanic or Latino  \\\n",
       "0                        0                                 1   \n",
       "1                        0                                 0   \n",
       "2                        0                                 1   \n",
       "3                        0                                 0   \n",
       "4                        0                                 1   \n",
       "\n",
       "   ethnicity_no answer  \n",
       "0                    0  \n",
       "1                    0  \n",
       "2                    0  \n",
       "3                    0  \n",
       "4                    0  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "birth_test_data_encoded = pd.get_dummies(birth_test_data, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_test_data_encoded = birth_test_data_encoded.drop(columns=columns_to_drop)\n",
    "birth_test_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate SPD and Equalized Odds on Training Data Holdout set using loop to access folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No module named 'tempeh': LawSchoolGPADataset will be unavailable. To install, run:\n",
      "pip install 'aif360[LawSchoolGPA]'\n",
      "WARNING:root:No module named 'fairlearn': ExponentiatedGradientReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "WARNING:root:No module named 'fairlearn': GridSearchReduction will be unavailable. To install, run:\n",
      "pip install 'aif360[Reductions]'\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/aif360/metrics/classification_metric.py:278: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  TPR=TP / P, TNR=TN / N, FPR=FP / N, FNR=FN / P,\n",
      "/home/jupyter/.local/lib/python3.7/site-packages/aif360/metrics/classification_metric.py:279: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  GTPR=GTP / P, GTNR=GTN / N, GFPR=GFP / N, GFNR=GFN / P,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean Difference (race_Black or African American): -0.097\n",
      " Equalized Odds Ratio (race_Black or African American): 0.0\n",
      " Mean Difference (race_None of these): 0.123\n",
      " Equalized Odds Ratio (race_None of these): nan\n",
      " Mean Difference (race_no answer): -0.008\n",
      " Equalized Odds Ratio (race_no answer): 0.0\n",
      " Mean Difference (race_More than one population): -0.07\n",
      " Equalized Odds Ratio (race_More than one population): 0.0\n",
      " Mean Difference (race_Asian): -0.091\n",
      " Equalized Odds Ratio (race_Asian): 0.0\n",
      " Mean Difference (race_Middle Eastern or North African): 0.123\n",
      " Equalized Odds Ratio (race_Middle Eastern or North African): nan\n",
      " Mean Difference (race_Native Hawaiian or Other Pacific Islander): 0.123\n",
      " Equalized Odds Ratio (race_Native Hawaiian or Other Pacific Islander): nan\n"
     ]
    }
   ],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming total_folds is the number of folds from your previous code\n",
    "total_folds = kfold.get_n_splits()\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in range(total_folds):\n",
    "    \n",
    "    # Use the predictions from the corresponding fold\n",
    "    svc_pred_fold = svc_predictions[fold]\n",
    "\n",
    "    # Iterate over minority groups\n",
    "    for minority_group in race_categories:\n",
    "        \n",
    "        # Create a new BinaryLabelDataset for each minority group within the fold\n",
    "        label_column_name = 'birth_class_binary'\n",
    "        bld = BinaryLabelDataset(\n",
    "            favorable_label=0, unfavorable_label=1,\n",
    "            df=birth_ho_encoded, label_names=[label_column_name],\n",
    "            protected_attribute_names=['race_White'] + race_categories\n",
    "        )\n",
    "\n",
    "        privileged_groups = [{'race_White': 1}]\n",
    "        unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "        # Create an instance of BinaryLabelDatasetMetric\n",
    "        metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "        # Calculate mean difference\n",
    "        mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "        # Assuming you have ground truth labels and predicted labels\n",
    "        cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "        # Calculate equalized odds ratio\n",
    "        equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {np.mean(mean_diffs[minority_group]).round(3)}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {np.mean(equalized_odds_ratios[minority_group]).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics SPD and Equalized Odds on Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Mean Difference (race_Black or African American): [-0.0760584414058193]\n",
      " Equalized Odds Ratio (race_Black or African American): [0.0]\n",
      " Mean Difference (race_None of these): [0.023545032914422448]\n",
      " Equalized Odds Ratio (race_None of these): [0.0]\n",
      " Mean Difference (race_no answer): [-0.006739496844777926]\n",
      " Equalized Odds Ratio (race_no answer): [0.0]\n",
      " Mean Difference (race_More than one population): [-0.04018528204620753]\n",
      " Equalized Odds Ratio (race_More than one population): [0.0]\n",
      " Mean Difference (race_Asian): [0.00970009620556167]\n",
      " Equalized Odds Ratio (race_Asian): [0.0]\n",
      " Mean Difference (race_Middle Eastern or North African): [0.09097924344073827]\n",
      " Equalized Odds Ratio (race_Middle Eastern or North African): [0.0]\n",
      " Mean Difference (race_Native Hawaiian or Other Pacific Islander): [0.02638594200533151]\n",
      " Equalized Odds Ratio (race_Native Hawaiian or Other Pacific Islander): [0.0]\n"
     ]
    }
   ],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "\n",
    "# Iterate over minority groups\n",
    "for minority_group in race_categories:\n",
    "\n",
    "    # Create a new BinaryLabelDataset for each minority group\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_test_data_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White'] + race_categories\n",
    "    )\n",
    "\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate mean difference\n",
    "    mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "    # Assuming you have ground truth labels and predicted labels\n",
    "    cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    # Calculate equalized odds ratio\n",
    "    equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {mean_diffs[minority_group]}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {equalized_odds_ratios[minority_group]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ground truth SPD calculation (this applies to entire dataset to see what our actual disparities are); no pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Statistical Parity Difference (Ground Truth): race_Black or African American -0.07805475596494593\n",
      " Statistical Parity Difference (Ground Truth): race_None of these -0.03565391540807694\n",
      " Statistical Parity Difference (Ground Truth): race_no answer -0.007601367643023993\n",
      " Statistical Parity Difference (Ground Truth): race_More than one population -0.046599678536843436\n",
      " Statistical Parity Difference (Ground Truth): race_Asian 0.001698330454807162\n",
      " Statistical Parity Difference (Ground Truth): race_Middle Eastern or North African 0.059700369190790914\n",
      " Statistical Parity Difference (Ground Truth): race_Native Hawaiian or Other Pacific Islander -0.01893659018011351\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "for minority_group in race_categories:\n",
    "    \n",
    "\n",
    "    # Create a BinaryLabelDataset for ground truth labels\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld_ground_truth = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White', minority_group]\n",
    "    )\n",
    "\n",
    "    # Set privileged and unprivileged groups\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld_ground_truth = BinaryLabelDatasetMetric(bld_ground_truth, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate SPD for ground truth labels\n",
    "    spd_ground_truth = metric_bld_ground_truth.statistical_parity_difference()\n",
    "              \n",
    "\n",
    "    print(f\" Statistical Parity Difference (Ground Truth):\", minority_group, spd_ground_truth)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
