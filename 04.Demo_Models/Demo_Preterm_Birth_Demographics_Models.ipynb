{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import demo dataframe\n",
    "\n",
    "Since the *All of Us* data is restricted, we have provided a demo data frame with fake data to test the code for the models\n",
    "\n",
    "We now import the relevant dataframe and rename it `birth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "print(\"Thanks for your patience while I import the dataframe.\")\n",
    "from rawdemodataframe import *\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "## For plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "## This sets the plot style\n",
    "## to have a grid on a white background\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "#from sklearn.metrics import precision_score\n",
    "#from sklearn.metrics import accuracy_score\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Demographics Dataset from Data_Cleaning_Demographics_FINAL\n",
    "\n",
    "- final dataframe is 'birth'\n",
    "- response variable is 'birth_class_binary'\n",
    "- 'preprocessor' is pipeline for one-hot encoding race and ethnicity and scaling numerical features  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test splits\n",
    "\n",
    "- Stratified splits\n",
    "The preterm birth rate in the United States is about 10%. Our data reflects this (although slightly higher), so we will be using stratified splits to ensure that our training and test sets both contain preterm births. \n",
    "  \n",
    "    \n",
    "- Multiple observations\n",
    "We are using multiple observations per person. To avoid using future data to predict the past, the *last* birth for each person will be reserved for the test set only. The training set may contain all births up to the last birth, including the first birth if it is the only birth. This is not a perfect solution to avoid data leakage, but the first thing we are trying. (*Note that this didn't end up being an issue here because of data quality.)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mark births that are the last birth of multiples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming 'birth' DataFrame has columns 'person_id' and 'birth_order'\n",
    "\n",
    "# Sort the DataFrame by 'person_id' and 'birth_order'\n",
    "birth = birth.sort_values(by=['person_id', 'birth_order'])\n",
    "\n",
    "# Identify the last birth in each group (person_id) with more than one birth\n",
    "last_births = birth[birth.duplicated(subset='person_id', keep=False) & ~birth.duplicated(subset='person_id', keep='last')]\n",
    "\n",
    "# Create a new column 'last_birth_of_multiples' and mark the identified last births as 1\n",
    "birth['last_birth_of_multiples'] = 0\n",
    "birth.loc[last_births.index, 'last_birth_of_multiples'] = 1\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(birth[['person_id', 'birth_order', 'last_birth_of_multiples']])\n",
    "print(\"Number of last births of multiples\", birth['last_birth_of_multiples'].sum())\n",
    "\n",
    "# Check the distribution of term and preterm in this set \n",
    "filtered_mutliples_df = birth[birth['last_birth_of_multiples'] == 1]\n",
    "birth_class_percentage = filtered_mutliples_df['birth_class'].value_counts(normalize=True) * 100\n",
    "print(birth_class_percentage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Include marked last births only in the test set\n",
    "birth_test_data = birth[birth['last_birth_of_multiples'] == 1]\n",
    "\n",
    "# Exclude last births of multiples from the dataset\n",
    "birth_remaining = birth[birth['last_birth_of_multiples'] == 0]\n",
    "\n",
    "# Reset the index for the resulting DataFrames\n",
    "birth_test_data.reset_index(drop=True, inplace=True)\n",
    "birth_remaining.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# Stratify the remaining births for inclusion in the training set\n",
    "birth_train_data, birth_test_remaining = train_test_split(birth_remaining,\n",
    "                                                          test_size=0.2,\n",
    "                                                          shuffle=True,\n",
    "                                                          random_state=404,\n",
    "                                                          stratify=birth_remaining['birth_class'])\n",
    "\n",
    "\n",
    "# Append last_birth_test_set to birth_test_remaining\n",
    "birth_test_data = birth_test_remaining.append(birth_test_data)\n",
    "\n",
    "\n",
    "# Reset the index for the resulting DataFrames\n",
    "birth_train_data.reset_index(drop=True, inplace=True)\n",
    "birth_test_data.reset_index(drop=True, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   #### Training and Test Data Set names:\n",
    "   Test Data Set: birth_test_data  \n",
    "   Training Data Set: birth_train_data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View dataframes to check that multiples births loop worked as expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Birth Train Data:\")\n",
    "#print(birth_train_data.info())\n",
    "print(\"birth train unique vals\", birth_train_data['birth_order'].unique())\n",
    "\n",
    "print(\"\\nBirth Test Data:\")\n",
    "#print(birth_test_data.info())\n",
    "print(\"birth test unique vals\",birth_test_data['birth_order'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verifying train-test splits by inspecting percentage of preterm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Data\n",
    "birth_class_counts = birth_train_data['birth_class'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Training Data: counts of each birth class:\")\n",
    "print(birth_class_counts)\n",
    "\n",
    "# Calculate the percentage of preterm births\n",
    "preterm_percentage = (birth_class_counts['Preterm'] / birth_class_counts.sum()) * 100\n",
    "\n",
    "print(f\"Training Data: percentage of preterm births: {preterm_percentage:.2f}%\")\n",
    "\n",
    "\n",
    "#Test Data\n",
    "birth_class_counts = birth_test_data['birth_class'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Test Data: counts of each birth class:\")\n",
    "print(birth_class_counts)\n",
    "\n",
    "# Calculate the percentage of preterm births\n",
    "preterm_percentage = (birth_class_counts['Preterm'] / birth_class_counts.sum()) * 100\n",
    "\n",
    "print(f\"Test Data: percentage of preterm births: {preterm_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize train data\n",
    "normalized_value_counts = birth_train_data['birth_class'].value_counts(normalize=True)\n",
    "\n",
    "# Display the normalized value counts\n",
    "print(\"Normalized value counts:\")\n",
    "print(normalized_value_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "Based on our normalized value counts, our baseline model is a random coin flip with probability matching the likelihood of our preterm births (0.145)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import recall_score, precision_recall_curve, auc, f1_score\n",
    "\n",
    "# Initialize empty lists to store evaluation metrics\n",
    "baseline_recalls = []\n",
    "baseline_f1_scores = []\n",
    "baseline_pr_aucs = []\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "np.random.seed(404)\n",
    "\n",
    "# Perform 1000 random draws\n",
    "for obs in range(1000):\n",
    "    # Generate random binomial draws with probability 0.17\n",
    "    draw = np.random.binomial(n=1, p=0.145, size=len(birth_train_data))\n",
    "    \n",
    "    # Calculate recall score and append to the list\n",
    "    recall = recall_score(birth_train_data.birth_class_binary.values, draw)\n",
    "    baseline_recalls.append(recall)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_train_data.birth_class_binary.values, draw)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    baseline_pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Calculate F1 score and append to the list\n",
    "    f1 = f1_score(birth_train_data.birth_class_binary.values, draw)\n",
    "    baseline_f1_scores.append(f1)\n",
    "\n",
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are some statistics for the recall score of our baseline:\")\n",
    "print(\"Mean Recall - \" + str(round(np.mean(baseline_recalls), 3)))\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(baseline_f1_scores), 3)))\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(baseline_pr_aucs), 3)))\n",
    "print(\"Median Recall - \" + str(round(np.median(baseline_recalls), 3)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(baseline_f1_scores), 3)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(baseline_pr_aucs), 3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training: Setting up for models\n",
    "\n",
    "We will use stratified 10-fold cross validation to account for the smaller percentage of pre-term births"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kfold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold_splits = 10\n",
    "\n",
    "kfold = StratifiedKFold(kfold_splits, shuffle=True, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['assisted_income_zip', 'high_school_education_zip', 'median_income_zip',\n",
    "              'no_health_insurance_zip', 'poverty_zip', 'vacant_housing_zip', 'deprivation_index_zip', 'age_at_birth']\n",
    "\n",
    "model_feat.extend([\"race_person\", \"ethnicity_person\"])\n",
    "model_feat.extend([\"birth_order\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost model - decision tree classifier with 50 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize arrays to store evaluation metrics\n",
    "ab_recalls = np.zeros(kfold_splits)\n",
    "ab_f1 = np.zeros(kfold_splits)\n",
    "ab_pr_auc = np.zeros(kfold_splits)\n",
    "\n",
    "# Initialize an empty dictionary to store feature importances\n",
    "feature_importance_dict = {}\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(birth_train_data, birth_train_data.birth_class_binary):\n",
    "    birth_tt = birth_train_data.iloc[train_index]\n",
    "    birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "    # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "    adaboost_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('adaboost', AdaBoostClassifier(n_estimators=50, random_state=404))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on training data\n",
    "    adaboost_pipeline.fit(birth_tt[model_feat], birth_tt.birth_class_binary)\n",
    "        \n",
    "     # Get feature importances\n",
    "    feature_importances = adaboost_pipeline.named_steps['adaboost'].feature_importances_\n",
    "\n",
    "    # Access the feature names after preprocessing\n",
    "    preprocessed_columns = adaboost_pipeline.named_steps['preprocessor'].get_feature_names_out(input_features=model_feat)\n",
    "\n",
    "    # Print feature names and their importances\n",
    "    for feature, importance in zip(preprocessed_columns, feature_importances):\n",
    "        #print(f\"Feature: {feature}, Importance: {importance}\")\n",
    "\n",
    "        # Store feature importance in the dictionary\n",
    "        feature_importance_dict.setdefault(feature, []).append(importance)\n",
    "        \n",
    "    # Predict on the test data\n",
    "    ab_pred = adaboost_pipeline.predict(birth_ho[model_feat])\n",
    "    \n",
    "    # Calculate evaluation metrics using predictions on the test set\n",
    "    ab_recalls[counter] = recall_score(birth_ho.birth_class_binary, ab_pred)\n",
    "    ab_f1[counter] = f1_score(birth_ho.birth_class_binary, ab_pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    ab_precision, ab_recall, _ = precision_recall_curve(birth_ho.birth_class_binary, adaboost_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "    ab_pr_auc[counter] = auc(ab_recall, ab_precision)\n",
    "    \n",
    "    # Adjust counter for the next k-fold split\n",
    "    counter += 1\n",
    "    \n",
    "# Calculate mean feature importances across folds\n",
    "mean_feature_importances = {feature: np.mean(importances) for feature, importances in feature_importance_dict.items()}\n",
    "\n",
    "# Print mean feature importances\n",
    "for feature, mean_importance in mean_feature_importances.items():\n",
    "    print(f\"Mean Feature Importance: {feature}, Mean Importance: {mean_importance.round(3)}\")\n",
    "\n",
    "print(\"Mean recall:\", np.mean(ab_recalls).round(3))\n",
    "print(\"Mean F1:\", np.mean(ab_f1).round(3))\n",
    "print(\"Mean PR-AUC\", np.mean(ab_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features to keep\n",
    "\n",
    "We looked at feature importance from our AdaBoost model to inform feature selection for future models. The highest numbers are listed below. Ultimately, this did not improve model performance. \n",
    "\n",
    "**Mean Feature Importance: cat__race_person_Asian, Mean Importance: 0.006\n",
    "Mean Feature Importance: cat__race_person_Black or African American, Mean Importance: 0.033999999999999996\n",
    "Mean Feature Importance: cat__race_person_Middle Eastern or North African, Mean Importance: 0.019999999999999997\n",
    "Mean Feature Importance: cat__race_person_More than one population, Mean Importance: 0.019999999999999997\n",
    "Mean Feature Importance: cat__race_person_Native Hawaiian or Other Pacific Islander, Mean Importance: 0.002\n",
    "Mean Feature Importance: cat__race_person_None of these, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__race_person_White, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__race_person_no answer, Mean Importance: 0.0**\n",
    "\n",
    "Mean Feature Importance: cat__ethnicity_person_Hispanic or Latino, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__ethnicity_person_None of these, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__ethnicity_person_Not Hispanic or Latino, Mean Importance: 0.027999999999999997\n",
    "Mean Feature Importance: cat__ethnicity_person_no answer, Mean Importance: 0.014000000000000002\n",
    "\n",
    "Mean Feature Importance: cat__birth_order_1, Mean Importance: 0.008\n",
    "Mean Feature Importance: cat__birth_order_2, Mean Importance: 0.0\n",
    "Mean Feature Importance: cat__birth_order_3, Mean Importance: 0.014000000000000002\n",
    "Mean Feature Importance: cat__birth_order_4, Mean Importance: 0.0\n",
    "\n",
    "Mean Feature Importance: num__assisted_income_zip, Mean Importance: 0.054\n",
    "**Mean Feature Importance: num__high_school_education_zip, Mean Importance: 0.120**\n",
    "Mean Feature Importance: num__median_income_zip, Mean Importance: 0.062\n",
    "**Mean Feature Importance: num__no_health_insurance_zip, Mean Importance: 0.138**\n",
    "Mean Feature Importance: num__poverty_zip, Mean Importance: 0.102\n",
    "**Mean Feature Importance: num__vacant_housing_zip, Mean Importance: 0.144**\n",
    "Mean Feature Importance: num__deprivation_index_zip, Mean Importance: 0.060\n",
    "**Mean Feature Importance: num__age_at_birth, Mean Importance: 0.174**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update model features based on feature importance \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_feat = ['high_school_education_zip', 'no_health_insurance_zip', 'vacant_housing_zip', 'age_at_birth']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Update preprocessor to transform a smaller number of columns\n",
    "\n",
    "Updated pipeline for using a subset of features based on feature selection. Commented out as the subset of features performed worse than the full set of features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#numeric_columns = ['high_school_education_zip', 'no_health_insurance_zip', 'vacant_housing_zip', 'age_at_birth']\n",
    "\n",
    "\n",
    "#preprocessor = ColumnTransformer(\n",
    "#    transformers=[\n",
    "#          ('num', StandardScaler(), numeric_columns)\n",
    "#    ],\n",
    "#    remainder='passthrough'  # Keeps the non-categorical columns as they are\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost with Log Regression, 50 weak learners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initialize arrays to store evaluation metrics\n",
    "ab_recalls = np.zeros(kfold_splits)\n",
    "ab_f1 = np.zeros(kfold_splits)\n",
    "ab_pr_auc = np.zeros(kfold_splits)\n",
    "counter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(birth_train_data, birth_train_data['birth_class_binary']):\n",
    "    birth_tt = birth_train_data.iloc[train_index]\n",
    "    birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "    # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "    adaboost_pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('adaboost', AdaBoostClassifier(base_estimator=LogisticRegression(), n_estimators=50, random_state=404))\n",
    "    ])\n",
    "\n",
    "    # Fit the pipeline on training data\n",
    "    adaboost_pipeline.fit(birth_tt[model_feat], birth_tt['birth_class_binary'])\n",
    "\n",
    "    # Predict on the holdout data\n",
    "    ab_pred = adaboost_pipeline.predict(birth_ho[model_feat])\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    ab_recalls[counter] = recall_score(birth_ho['birth_class_binary'], ab_pred)\n",
    "    ab_f1[counter] = f1_score(birth_ho['birth_class_binary'], ab_pred)\n",
    "\n",
    "    # Calculate precision-recall curve and AUC\n",
    "    ab_precision, ab_recall, _ = precision_recall_curve(birth_ho['birth_class_binary'], adaboost_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "    ab_pr_auc[counter] = auc(ab_recall, ab_precision)\n",
    "\n",
    "    # Adjust counter for the next k-fold split\n",
    "    counter += 1\n",
    "\n",
    "print(\"Mean recall:\", np.mean(ab_recalls).round(3))\n",
    "print(\"Mean F1:\", np.mean(ab_f1).round(3))\n",
    "print(\"Mean PR-AUC:\", np.mean(ab_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC model with class weights and loop to print values for multiple class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import recall_score, f1_score, precision_recall_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "weights = [5.75] #add numbers to explore more weights\n",
    "\n",
    "for weight in weights:\n",
    "    \n",
    "    # Initialize arrays to store evaluation metrics\n",
    "    svc_recalls = np.zeros(kfold_splits)\n",
    "    svc_f1 = np.zeros(kfold_splits)\n",
    "    svc_pr_auc = np.zeros(kfold_splits)\n",
    "\n",
    "    # Initialize list to store predictions\n",
    "    svc_predictions = []\n",
    "\n",
    "    counter = 0\n",
    "\n",
    "    for train_index, test_index in kfold.split(birth_train_data, birth_train_data.birth_class_binary):\n",
    "        birth_tt = birth_train_data.iloc[train_index]\n",
    "        birth_ho = birth_train_data.iloc[test_index]\n",
    "\n",
    "        # Assuming class_weights is a dictionary containing class weights; preterm birth (1) should have heavier weight\n",
    "        class_weights = {0: 1, 1: weight}  \n",
    "    \n",
    "        # Add a column for class weights to birth_tt\n",
    "        birth_tt = birth_tt.copy()\n",
    "        birth_tt['class_weights'] = birth_tt['birth_class_binary'].map(class_weights)\n",
    "    \n",
    "        # Create the pipeline with preprocessing and AdaBoostClassifier\n",
    "        svc_pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('svc', SVC(probability=True, random_state=404))\n",
    "        ])\n",
    "\n",
    "        print(\"Now fitting the model. Please be patient.\")\n",
    "        # Fit the pipeline on training data\n",
    "        svc_pipeline.fit(birth_tt[model_feat], birth_tt['birth_class_binary'], svc__sample_weight=birth_tt['class_weights'])\n",
    "\n",
    "        # Predict on the holdout data\n",
    "        svc_pred = svc_pipeline.predict(birth_ho[model_feat])\n",
    "\n",
    "        # Append predictions to the list\n",
    "        svc_predictions.append(svc_pred)\n",
    "    \n",
    "        # Calculate evaluation metrics\n",
    "        svc_recalls[counter] = recall_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "        #print(\"fold\", counter, \"recall\", svc_recalls[counter])    \n",
    "        svc_f1[counter] = f1_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "        #print(\"fold\", counter, \"f1\", svc_f1[counter]) \n",
    "    \n",
    "        # Calculate precision-recall curve and AUC\n",
    "        svc_precision, svc_recall, _ = precision_recall_curve(birth_ho.birth_class_binary, svc_pipeline.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "        svc_pr_auc[counter] = auc(svc_recall, svc_precision)\n",
    "        #print(\"fold\", counter, \"pr-auc\", svc_pr_auc[counter]) \n",
    "    \n",
    "        # Adjust counter for the next k-fold split\n",
    "        counter += 1\n",
    "        #print(\"onto the next fold\") \n",
    "\n",
    "\n",
    "#print(\"Recalls\", svc_recalls)\n",
    "#print(\"F1 Scores\", svc_f1)\n",
    "#print(\"PR-AUC\", svc_pr_auc)\n",
    "    print(\"weight:\", weight)\n",
    "    print(\"Mean recall\", np.mean(svc_recalls).round(3))\n",
    "    print(\"Mean F1\", np.mean(svc_f1).round(3))\n",
    "    print(\"Mean PR-AUC\", np.mean(svc_pr_auc).round(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### I tried to maximize the PR-AUC, which is still performing below baseline. I'm not able to improve this model further with class weights alone. Basically the model is just over-predicting preterm births. These are some scores for \n",
    "\n",
    "model_feat = ['assisted_income_zip', 'high_school_education_zip', 'median_income_zip',\n",
    "              'no_health_insurance_zip', 'poverty_zip', 'vacant_housing_zip', 'deprivation_index_zip', 'age_at_birth']  \n",
    "              \n",
    "model_feat.extend([\"race_person\", \"ethnicity_person\"])  \n",
    "\n",
    "model_feat.extend([\"birth_order\"])  \n",
    "\n",
    "weight: 5  \n",
    "Mean recall 0.291\n",
    "Mean F1 0.237 \n",
    "Mean PR-AUC 0.182\n",
    "\n",
    "weight: 5.25  \n",
    "Mean recall 0.320\n",
    "Mean F1 0.241\n",
    "Mean PR-AUC 0.180  \n",
    "\n",
    "weight: 5.5  \n",
    "Mean recall 0.355 \n",
    "Mean F1 0.244\n",
    "Mean PR-AUC 0.179  \n",
    "\n",
    "**weight: 5.75  \n",
    "Mean recall 0.434  \n",
    "Mean F1 0.261  \n",
    "Mean PR-AUC 0.181**  \n",
    "\n",
    "weight: 6  \n",
    "Mean recall 0.464  \n",
    "Mean F1 0.250  \n",
    "Mean PR-AUC 0.179  \n",
    "\n",
    "weight: 6.25  \n",
    "Mean recall 0.521  \n",
    "Mean F1 0.253  \n",
    "Mean PR-AUC 0.177  \n",
    "\n",
    "weight: 6.5  \n",
    "Mean recall 0.581  \n",
    "Mean F1 0.255  \n",
    "Mean PR-AUC 0.174  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(birth_ho['birth_class_binary'], svc_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(birth_ho['birth_class_binary'], svc_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC with GridSearch CV\n",
    "\n",
    "Remembered there is a better way to tune hyperparameters than the loop above....\n",
    "\n",
    "Checked with:\n",
    "param_grid = {\n",
    "     'svc__C': [.1, 1, 10],  \n",
    "    'svc__kernel': ['linear', 'poly', 'rbf' ],\n",
    "    'svc__class_weight': [{0: 1, 1: weight} for weight in np.arange(5, 7, 0.25)]\n",
    "}\n",
    "\n",
    "param_grid below updated to best model in order to save computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "\n",
    "# Define the hyperparameter grid to search\n",
    "param_grid = {\n",
    "     'svc__C': [1],  \n",
    "    'svc__kernel': ['linear'],\n",
    "    'svc__class_weight': [{0: 1, 1: 5.75}]\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# Create pipeline using preprocessor and initial hyperparameter settings (must match hyperparameters in param grid)\n",
    "gridsearch_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc', SVC(C=1.0, kernel='linear', class_weight=None, probability=True, random_state=404)) \n",
    "])\n",
    "\n",
    "\n",
    "# Create the GridSearchCV object\n",
    "grid_search = GridSearchCV(estimator=gridsearch_pipeline, param_grid=param_grid, scoring='average_precision', cv=5)  \n",
    "print(\"Created the GridSearch SV object. Now fitting the model. Please be patient.\")\n",
    "\n",
    "# Fit the GridSearchCV object to your data\n",
    "grid_search.fit(birth_tt[model_feat], birth_tt['birth_class_binary'])\n",
    "\n",
    "# Print the best hyperparameters found\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Get the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting best hyperparameters for SVC model (class weights, kernel, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accessing hyperparameters\n",
    "best_kernel = best_model.named_steps['svc'].kernel\n",
    "best_C = best_model.named_steps['svc'].C\n",
    "best_class_weight = best_model.named_steps['svc'].class_weight\n",
    "\n",
    "# Print or use the values as needed\n",
    "print(\"Best Kernel:\", best_kernel)\n",
    "print(\"Best C:\", best_C)\n",
    "print(\"Best Class Weight:\", best_class_weight)\n",
    "\n",
    "\n",
    "#Make predictions on the test set using the best model\n",
    "gridsearch_pred = best_model.predict(birth_ho[model_feat])\n",
    "\n",
    "gridsearch_scores = best_model.decision_function(birth_ho[model_feat])\n",
    "\n",
    "\n",
    "# Print scores\n",
    "pr_auc = average_precision_score(birth_ho['birth_class_binary'], gridsearch_scores)\n",
    "print(\"PR AUC of the Best Model:\", pr_auc.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(birth_ho['birth_class_binary'], gridsearch_pred))\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\\n\", classification_report(birth_ho['birth_class_binary'], gridsearch_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing: SVC model with final hyperparameters and pipeline\n",
    "\n",
    "- Chose 6.25; highest PR-AUC achieved  \n",
    "\n",
    "Recalls 0.5492957746478874  \n",
    "F1 Scores 0.23330009970089732  \n",
    "PR-AUC 0.15802294790524057  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Add class weights \n",
    "class_weights = {0: 1, 1: 5.75}  \n",
    "birth_test_data = birth_test_data.copy()\n",
    "birth_test_data['class_weights'] = birth_test_data['birth_class_binary'].map(class_weights)\n",
    "\n",
    "# Predict on the test data\n",
    "svc_pred_final = svc_pipeline.predict(birth_test_data[model_feat])\n",
    "        \n",
    "# Calculate evaluation metrics\n",
    "svc_recalls = recall_score(birth_test_data.birth_class_binary.values, svc_pred_final)   \n",
    "svc_f1 = f1_score(birth_test_data.birth_class_binary.values, svc_pred_final)\n",
    "    \n",
    "# Calculate precision-recall curve and AUC\n",
    "svc_precision, svc_recall, _ = precision_recall_curve(birth_test_data.birth_class_binary, svc_pipeline.predict_proba(birth_test_data[model_feat])[:, 1])\n",
    "svc_pr_auc = auc(svc_recall, svc_precision)\n",
    "    \n",
    "\n",
    "print(\"Recalls\", svc_recalls.round(3))\n",
    "print(\"F1 Scores\", svc_f1.round(3))\n",
    "print(\"PR-AUC\", svc_pr_auc.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for fairness metrics because I couldn't get the pipeline to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_encoded - entire birth dataframe for metrics on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_encoded = pd.get_dummies(birth, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "# Drop datetime and obj col as it causes errors and is not necessary\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_encoded = birth_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "birth_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_ho encoding (birth holdout set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_ho_encoded = pd.get_dummies(birth_ho, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_ho_encoded = birth_ho_encoded.drop(columns=columns_to_drop)\n",
    "birth_ho_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_test_data (Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_test_data_encoded = pd.get_dummies(birth_test_data, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class']\n",
    "birth_test_data_encoded = birth_test_data_encoded.drop(columns=columns_to_drop)\n",
    "birth_test_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate SPD and Equalized Odds on Training Data Holdout set using loop to access folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming total_folds is the number of folds from your previous code\n",
    "total_folds = kfold.get_n_splits()\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in range(total_folds):\n",
    "    \n",
    "    # Use the predictions from the corresponding fold\n",
    "    svc_pred_fold = svc_predictions[fold]\n",
    "\n",
    "    # Iterate over minority groups\n",
    "    for minority_group in race_categories:\n",
    "        \n",
    "        # Create a new BinaryLabelDataset for each minority group within the fold\n",
    "        label_column_name = 'birth_class_binary'\n",
    "        bld = BinaryLabelDataset(\n",
    "            favorable_label=0, unfavorable_label=1,\n",
    "            df=birth_ho_encoded, label_names=[label_column_name],\n",
    "            protected_attribute_names=['race_White'] + race_categories\n",
    "        )\n",
    "\n",
    "        privileged_groups = [{'race_White': 1}]\n",
    "        unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "        # Create an instance of BinaryLabelDatasetMetric\n",
    "        metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "        # Calculate mean difference\n",
    "        mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "        # Assuming you have ground truth labels and predicted labels\n",
    "        cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "        # Calculate equalized odds ratio\n",
    "        equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {np.mean(mean_diffs[minority_group]).round(3)}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {np.mean(equalized_odds_ratios[minority_group]).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics SPD and Equalized Odds on Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "\n",
    "# Iterate over minority groups\n",
    "for minority_group in race_categories:\n",
    "\n",
    "    # Create a new BinaryLabelDataset for each minority group\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_test_data_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White'] + race_categories\n",
    "    )\n",
    "\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate mean difference\n",
    "    mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "    # Assuming you have ground truth labels and predicted labels\n",
    "    cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    # Calculate equalized odds ratio\n",
    "    equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {mean_diffs[minority_group]}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {equalized_odds_ratios[minority_group]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Ground truth SPD calculation (this applies to entire dataset to see what our actual disparities are); no pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "for minority_group in race_categories:\n",
    "    \n",
    "\n",
    "    # Create a BinaryLabelDataset for ground truth labels\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld_ground_truth = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White', minority_group]\n",
    "    )\n",
    "\n",
    "    # Set privileged and unprivileged groups\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld_ground_truth = BinaryLabelDatasetMetric(bld_ground_truth, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate SPD for ground truth labels\n",
    "    spd_ground_truth = metric_bld_ground_truth.statistical_parity_difference()\n",
    "              \n",
    "\n",
    "    print(f\" Statistical Parity Difference (Ground Truth):\", minority_group, spd_ground_truth)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
