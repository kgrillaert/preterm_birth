{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import demo dataframe\n",
    "\n",
    "Since the *All of Us* data is restricted, we have provided a demo data frame with fake data to test the code for the models\n",
    "\n",
    "We now import the relevant dataframe and rename it `birth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run as needed\n",
    "!pip install import-ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import import_ipynb\n",
    "print(\"Thanks for your patience while I import the dataframe.\")\n",
    "from rawlifestyledataframe import *\n",
    "print(\"All done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import recall_score, precision_score, f1_score, precision_recall_curve, auc\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize Response Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth['birth_class_binary'] = 0\n",
    "birth.loc[birth.birth_class=='Preterm', 'birth_class_binary'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and testing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a stratified split because we have largely imbalanced classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_train, birth_test = train_test_split(birth.copy(),\n",
    "                                          shuffle=True,\n",
    "                                          random_state=650,\n",
    "                                          stratify=birth['birth_class'],\n",
    "                                          test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find proportion of response variable that falls into either category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_value_counts = birth_train['birth_class'].value_counts(normalize=True)\n",
    "\n",
    "# Display the normalized value counts\n",
    "print(\"Normalized value counts:\")\n",
    "print(normalized_value_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `birth_train`, Term births are 86.3% and Preterm births are 13.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store evaluation metrics\n",
    "baseline_precision = []\n",
    "baseline_recalls = []\n",
    "baseline_f1_scores = []\n",
    "baseline_pr_aucs = []\n",
    "\n",
    "# Perform 1000 random draws\n",
    "for obs in range(1000):\n",
    "    \n",
    "    # Generate random binomial draws with probability 0.137\n",
    "    draw = np.random.binomial(n=1, p=0.137, size=len(birth_train))\n",
    "  \n",
    "    # Calculate precision score and append to the list\n",
    "    precision_obs = precision_score(birth_train.birth_class_binary, draw)\n",
    "    baseline_precision.append(precision_obs)\n",
    "\n",
    "    # Calculate recall score and append to the list\n",
    "    recall_obs = recall_score(birth_train.birth_class_binary, draw)\n",
    "    baseline_recalls.append(recall_obs)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_train.birth_class_binary, draw)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    baseline_pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Calculate F1 score and append to the list\n",
    "    f1 = f1_score(birth_train.birth_class_binary, draw)\n",
    "    baseline_f1_scores.append(f1)\n",
    "\n",
    "plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are some statistics for our baseline:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(baseline_precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(baseline_precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(baseline_recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(baseline_recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(baseline_f1_scores), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(baseline_f1_scores), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(baseline_pr_aucs), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(baseline_pr_aucs), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to scale and one-hot encode all features in `birth_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of all categorical features\n",
    "\n",
    "num_cols = ['maternal_age', 'BMI']\n",
    "\n",
    "cat_cols = ['smoking_status', 'drinking_frequency', 'six_or_more_drinks_occurrence', \n",
    "                     'birth_order', 'diabetes', 'mental_health', 'drug_use']\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "                                               ('num', StandardScaler(), num_cols)],\n",
    "                                 remainder='passthrough')\n",
    "\n",
    "birth_train_encoded = preprocessor.fit_transform(birth_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result back to a DataFrame for easier inspection\n",
    "columns_after_encoding = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Custom function to remove prefixes\n",
    "def remove_prefix(prefix, column_names):\n",
    "    return [name[len(prefix):] if name.startswith(prefix) else name for name in column_names]\n",
    "\n",
    "columns_after_encoding = remove_prefix(\"cat__\", columns_after_encoding)\n",
    "columns_after_encoding = remove_prefix(\"num__\", columns_after_encoding)\n",
    "columns_after_encoding = remove_prefix(\"remainder__\", columns_after_encoding)\n",
    "\n",
    "birth_train_encoded = pd.DataFrame(birth_train_encoded, columns=columns_after_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "birth_train_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all data types are integers as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = ['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes',\n",
    "                   'birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other']\n",
    "\n",
    "# Make sure all encoded variables are integer types\n",
    "birth_train_encoded.birth_class_binary = birth_train_encoded.birth_class_binary.astype(int)\n",
    "birth_train_encoded[cat_feat] = birth_train_encoded[cat_feat].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_train_encoded.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with No Interactions, all features included"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These include the following variables:\n",
    "\n",
    "Continuous:\n",
    "- Maternal age\n",
    "- BMI\n",
    "\n",
    "Categorical:\n",
    "- Smoking Status\n",
    "- Drinking frequency\n",
    "- Binge drinking\n",
    "- Drug use\n",
    "- Diabetes\n",
    "- Mental health\n",
    "- Birth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['maternal_age', 'BMI']\n",
    "\n",
    "# Add one-hot encoded lifestyle features\n",
    "model_feat.extend(['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes'])\n",
    "\n",
    "# Add one-hot encoded health features\n",
    "model_feat.extend(['birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stratified KFold Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 10-fold cross-validation due to the imbalanced class sizes in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=123)\n",
    "\n",
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize arrays to store evaluation metrics\n",
    "prec = np.zeros(num_splits)\n",
    "recalls = np.zeros(num_splits)\n",
    "f1 = np.zeros(num_splits)\n",
    "pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# Start counter\n",
    "counter = 0\n",
    "\n",
    "# Loop through each KFold split\n",
    "for train_index, test_index in kfold.split(birth_train_encoded, birth_train_encoded.birth_class_binary):\n",
    "    birth_tt = birth_train_encoded.iloc[train_index]\n",
    "    birth_ho = birth_train_encoded.iloc[test_index]\n",
    "\n",
    "    log_reg = LogisticRegression(penalty='l2', class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "    log_reg.fit(birth_tt[model_feat].values, birth_tt.birth_class_binary.values)\n",
    "        \n",
    "    pred = log_reg.predict(birth_ho[model_feat].values)\n",
    "    \n",
    "    prec[counter] = precision_score(birth_ho.birth_class_binary.values, pred)\n",
    "    recalls[counter] = recall_score(birth_ho.birth_class_binary.values, pred)\n",
    "    f1[counter] = f1_score(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_ho.birth_class_binary.values, log_reg.predict_proba(birth_ho[model_feat].values)[:,1])\n",
    "    pr_auc[counter] = auc(recall, precision)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    print('---Metrics for Fold ', counter+1,'---')\n",
    "    print('Precision: ', prec[counter])\n",
    "    print('Recall: ', recalls[counter])\n",
    "    print('F1: ', f1[counter])\n",
    "    print('AUC: ', pr_auc[counter])\n",
    "    print('Confusion Matrix: ')\n",
    "    print(conf_mat)\n",
    "    print(' ')\n",
    "    \n",
    "    plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    #Adjust counter for next k-fold split\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are the statistics for the recall score of our model:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(f1), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(f1), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(pr_auc), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(pr_auc), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with No Interactions, selected features only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on EDA, we will select only features that gave a clear difference between preterm and term to include in our model.\n",
    "\n",
    "These include the following variables:\n",
    "\n",
    "Categorical:\n",
    "- Drug use\n",
    "- Diabetes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['drug_use_No',\n",
    "                'drug_use_Yes',\n",
    "                'diabetes_None',\n",
    "                'diabetes_Type1',\n",
    "                'diabetes_Type2',\n",
    "                'diabetes_TypeUnknown']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stratified KFold Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 10-fold cross-validation due to the imbalanced class sizes in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=123)\n",
    "\n",
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize arrays to store evaluation metrics\n",
    "prec = np.zeros(num_splits)\n",
    "recalls = np.zeros(num_splits)\n",
    "f1 = np.zeros(num_splits)\n",
    "pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# Start counter\n",
    "counter = 0\n",
    "\n",
    "# Loop through each KFold split\n",
    "for train_index, test_index in kfold.split(birth_train_encoded, birth_train_encoded.birth_class_binary):\n",
    "    birth_tt = birth_train_encoded.iloc[train_index]\n",
    "    birth_ho = birth_train_encoded.iloc[test_index]\n",
    "\n",
    "    log_reg = LogisticRegression(penalty='l2', class_weight='balanced', max_iter=1000, random_state=456)\n",
    "        \n",
    "    log_reg.fit(birth_tt[model_feat].values, birth_tt.birth_class_binary.values)\n",
    "        \n",
    "    pred = log_reg.predict(birth_ho[model_feat].values)\n",
    "    \n",
    "    prec[counter] = precision_score(birth_ho.birth_class_binary.values, pred)\n",
    "    recalls[counter] = recall_score(birth_ho.birth_class_binary.values, pred)\n",
    "    f1[counter] = f1_score(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_ho.birth_class_binary.values, log_reg.predict_proba(birth_ho[model_feat].values)[:,1])\n",
    "    pr_auc[counter] = auc(recall, precision)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    print('---Metrics for Fold ', counter+1,'---')\n",
    "    print('Precision: ', prec[counter])\n",
    "    print('Recall: ', recalls[counter])\n",
    "    print('F1: ', f1[counter])\n",
    "    print('AUC: ', pr_auc[counter])\n",
    "    print('Confusion Matrix: ')\n",
    "    print(conf_mat)\n",
    "    print(' ')\n",
    "    \n",
    "    plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    #Adjust counter for next k-fold split\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are the statistics for the recall score of our model:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(f1), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(f1), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(pr_auc), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(pr_auc), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression with Interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will only use interactions between continuous variables, because there are not enough observations in each of the categorical variables to include interaction terms.\n",
    "\n",
    "These include the following variables:\n",
    "\n",
    "Continuous:\n",
    "- Maternal age\n",
    "- BMI\n",
    "\n",
    "Categorical:\n",
    "- Smoking Status\n",
    "- Drinking frequency\n",
    "- Binge drinking\n",
    "- Drug use\n",
    "- Diabetes\n",
    "- Mental health\n",
    "- Birth order\n",
    "\n",
    "Interactions: \n",
    "- Maternal age & BMI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create interaction features and add to train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify which features for interaction\n",
    "interaction_features = ['maternal_age', 'BMI']\n",
    "\n",
    "# Create a pipeline with PolynomialFeatures\n",
    "degree = 2  # Set the degree of interaction terms\n",
    "poly = PolynomialFeatures(degree, interaction_only=True, include_bias=False)\n",
    "birth_train_poly = poly.fit_transform(birth_train_encoded[interaction_features])\n",
    "\n",
    "# Concatenate the polynomial features with the original features\n",
    "birth_train = pd.concat([birth_train_encoded, pd.DataFrame(birth_train_poly, columns=poly.get_feature_names_out(interaction_features))], axis=1)\n",
    "#birth_train = birth_train.drop(columns=birth_train.columns[[-2, -3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define all features for model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['maternal_age', 'BMI']\n",
    "\n",
    "# Add one-hot encoded lifestyle features\n",
    "model_feat.extend(['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes'])\n",
    "\n",
    "# Add one-hot encoded health features\n",
    "model_feat.extend(['birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other'])\n",
    "\n",
    "# Add interaction term\n",
    "model_feat.extend(['maternal_age BMI'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stratified KFold Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 10-fold cross-validation due to the imbalanced class sizes in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=123)\n",
    "\n",
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize arrays to store evaluation metrics\n",
    "prec = np.zeros(num_splits)\n",
    "recalls = np.zeros(num_splits)\n",
    "f1 = np.zeros(num_splits)\n",
    "pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# Start counter\n",
    "counter = 0\n",
    "\n",
    "# Loop through each KFold split\n",
    "for train_index, test_index in kfold.split(birth_train, birth_train.birth_class_binary):\n",
    "    birth_tt = birth_train.iloc[train_index]\n",
    "    birth_ho = birth_train.iloc[test_index]\n",
    "\n",
    "    log_reg = LogisticRegression(penalty='l2', class_weight='balanced', max_iter=1000)\n",
    "        \n",
    "    log_reg.fit(birth_tt[model_feat].values, birth_tt.birth_class_binary.values)\n",
    "        \n",
    "    pred = log_reg.predict(birth_ho[model_feat].values)\n",
    "    \n",
    "    prec[counter] = precision_score(birth_ho.birth_class_binary.values, pred)\n",
    "    recalls[counter] = recall_score(birth_ho.birth_class_binary.values, pred)\n",
    "    f1[counter] = f1_score(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_ho.birth_class_binary.values, log_reg.predict_proba(birth_ho[model_feat].values)[:,1])\n",
    "    pr_auc[counter] = auc(recall, precision)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    print('---Metrics for Fold ', counter+1,'---')\n",
    "    print('Precision: ', prec[counter])\n",
    "    print('Recall: ', recalls[counter])\n",
    "    print('F1: ', f1[counter])\n",
    "    print('AUC: ', pr_auc[counter])\n",
    "    print('Confusion Matrix: ')\n",
    "    print(conf_mat)\n",
    "    print(' ')\n",
    "    \n",
    "    plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    #Adjust counter for next k-fold split\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are the statistics for the recall score of our model:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(f1), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(f1), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(pr_auc), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(pr_auc), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the features that gave us the best metrics when using logistic regression.\n",
    "\n",
    "These include the following variables:\n",
    "\n",
    "Continuous:\n",
    "- Maternal age\n",
    "- BMI\n",
    "\n",
    "Categorical:\n",
    "- Smoking Status\n",
    "- Drinking frequency\n",
    "- Binge drinking\n",
    "- Drug use\n",
    "- Diabetes\n",
    "- Mental health\n",
    "- Birth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['maternal_age', 'BMI']\n",
    "\n",
    "# Add one-hot encoded lifestyle features\n",
    "model_feat.extend(['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes'])\n",
    "\n",
    "# Add one-hot encoded health features\n",
    "model_feat.extend(['birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stratified KFold Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 10-fold cross-validation due to the imbalanced class sizes in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = StratifiedKFold(n_splits=10,\n",
    "                        shuffle=True,\n",
    "                        random_state=123)\n",
    "\n",
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Define weight as inversely proportional to class frequencies (in SVC documentation)\n",
    "weights = len(birth_train_encoded) / (2 * np.bincount(birth_train_encoded.birth_class_binary))\n",
    "\n",
    "# Initialize arrays to store evaluation metrics\n",
    "svc_prec = np.zeros(num_splits)\n",
    "svc_recalls = np.zeros(num_splits)\n",
    "svc_f1 = np.zeros(num_splits)\n",
    "svc_pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# Initialize list to store predictions\n",
    "svc_predictions = []\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for train_index, test_index in kfold.split(birth_train_encoded, birth_train_encoded.birth_class_binary):\n",
    "    birth_tt = birth_train_encoded.iloc[train_index]\n",
    "    birth_ho = birth_train_encoded.iloc[test_index]\n",
    "\n",
    "    # Create dictionary for class weights, with preterm birth having a heavier weight\n",
    "    class_weights = {0: weights[0], 1: weights[1]}  \n",
    "\n",
    "    # Add a column for class weights to birth_tt\n",
    "    birth_tt = birth_tt.copy()\n",
    "    birth_tt['class_weights'] = birth_tt['birth_class_binary'].map(class_weights)\n",
    "\n",
    "    # Create the SVC\n",
    "    svc = SVC(probability=True, random_state=404, class_weight='balanced')\n",
    "\n",
    "    # Fit the pipeline on training data\n",
    "    svc.fit(birth_tt[model_feat].values, birth_tt.birth_class_binary.values) \n",
    "    #        sample_weight=birth_tt.class_weights.values)\n",
    "\n",
    "    # Predict on the holdout data\n",
    "    svc_pred = svc.predict(birth_ho[model_feat].values)\n",
    "\n",
    "    # Append predictions to the list\n",
    "    svc_predictions.append(svc_pred)\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    svc_prec[counter] = precision_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "    svc_recalls[counter] = recall_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "    svc_f1[counter] = f1_score(birth_ho.birth_class_binary.values, svc_pred)\n",
    "\n",
    "    # Calculate precision-recall curve and AUC\n",
    "    svc_precision, svc_recall, _ = precision_recall_curve(birth_ho.birth_class_binary, \n",
    "                                                          svc.predict_proba(birth_ho[model_feat])[:, 1])\n",
    "    svc_pr_auc[counter] = auc(svc_recall, svc_precision)\n",
    "\n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(birth_ho.birth_class_binary.values, svc_pred)\n",
    "\n",
    "    # Print all metrics\n",
    "    print('---Metrics for Fold ', counter+1,'---')\n",
    "    print('Precision: ', svc_prec[counter])\n",
    "    print('Recall: ', svc_recalls[counter])\n",
    "    print('F1: ', svc_f1[counter])\n",
    "    print('AUC: ', svc_pr_auc[counter])\n",
    "    print('Confusion Matrix: ')\n",
    "    print(conf_mat)\n",
    "    print(' ')\n",
    "    \n",
    "    plt.plot(svc_recall, svc_precision, marker='.', label='SVC')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "\n",
    "    # Adjust counter for the next k-fold split\n",
    "    counter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are the statistics for the recall score of our model:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(svc_prec), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(svc_prec), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(svc_recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(svc_recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(svc_f1), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(svc_f1), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(svc_pr_auc), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(svc_pr_auc), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ROSE Method for Unbalanced Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install imbalanced learn package and import libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only need to do this once\n",
    "!pip install -U imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### use RandomOverSampler to oversample minority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original dataset: ')\n",
    "print(birth_train_encoded.birth_class_binary.value_counts())\n",
    "print('')\n",
    "\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "\n",
    "birth_train_rose, birth_train_rose.birth_class_binary = ros.fit_resample(birth_train_encoded, birth_train_encoded.birth_class_binary)\n",
    "\n",
    "print('Oversampled dataset: ')\n",
    "print(birth_train_rose.birth_class_binary.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a new baseline model, with oversampled minority class data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find proportion of response variable that falls into either category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_value_counts = birth_train_rose['birth_class'].value_counts(normalize=True)\n",
    "\n",
    "# Display the normalized value counts\n",
    "print(\"Normalized value counts:\")\n",
    "print(normalized_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialize empty lists to store evaluation metrics\n",
    "baseline_precision = []\n",
    "baseline_recalls = []\n",
    "baseline_f1_scores = []\n",
    "baseline_pr_aucs = []\n",
    "\n",
    "# Perform 1000 random draws\n",
    "for obs in range(1000):\n",
    "    \n",
    "    # Generate random binomial draws with probability 0.5\n",
    "    draw = np.random.binomial(n=1, p=0.5, size=len(birth_train_rose))\n",
    "  \n",
    "    # Calculate precision score and append to the list\n",
    "    precision_obs = precision_score(birth_train_rose.birth_class_binary, draw)\n",
    "    baseline_precision.append(precision_obs)\n",
    "\n",
    "    # Calculate recall score and append to the list\n",
    "    recall_obs = recall_score(birth_train_rose.birth_class_binary, draw)\n",
    "    baseline_recalls.append(recall_obs)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_train_rose.birth_class_binary, draw)\n",
    "    pr_auc = auc(recall, precision)\n",
    "    baseline_pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Calculate F1 score and append to the list\n",
    "    f1 = f1_score(birth_train_rose.birth_class_binary, draw)\n",
    "    baseline_f1_scores.append(f1)\n",
    "\n",
    "plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are some statistics for our baseline:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(baseline_precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(baseline_precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(baseline_recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(baseline_recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(baseline_f1_scores), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(baseline_f1_scores), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(baseline_pr_aucs), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(baseline_pr_aucs), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the features that gave us the best metrics when using logistic regression.\n",
    "\n",
    "These include the following variables:\n",
    "\n",
    "Continuous:\n",
    "- Maternal age\n",
    "- BMI\n",
    "\n",
    "Categorical:\n",
    "- Smoking Status\n",
    "- Drinking frequency\n",
    "- Binge drinking\n",
    "- Drug use\n",
    "- Diabetes\n",
    "- Mental health\n",
    "- Birth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['maternal_age', 'BMI']\n",
    "\n",
    "# Add one-hot encoded lifestyle features\n",
    "model_feat.extend(['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes'])\n",
    "\n",
    "# Add one-hot encoded health features\n",
    "model_feat.extend(['birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Stratified KFold Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will perform a stratified 10-fold cross-validation due to the imbalanced class sizes in the response variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=123)\n",
    "\n",
    "num_splits = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize arrays to store evaluation metrics\n",
    "prec = np.zeros(num_splits)\n",
    "recalls = np.zeros(num_splits)\n",
    "f1 = np.zeros(num_splits)\n",
    "pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# Start counter\n",
    "counter = 0\n",
    "\n",
    "# Loop through each KFold split\n",
    "for train_index, test_index in kfold.split(birth_train_rose, birth_train_rose.birth_class_binary):\n",
    "    birth_tt = birth_train_rose.iloc[train_index]\n",
    "    birth_ho = birth_train_rose.iloc[test_index]\n",
    "\n",
    "    log_reg = LogisticRegression(max_iter=1000)\n",
    "        \n",
    "    log_reg.fit(birth_tt[model_feat].values, birth_tt.birth_class_binary.values)\n",
    "        \n",
    "    pred = log_reg.predict(birth_ho[model_feat].values)\n",
    "    \n",
    "    prec[counter] = precision_score(birth_ho.birth_class_binary.values, pred)\n",
    "    recalls[counter] = recall_score(birth_ho.birth_class_binary.values, pred)\n",
    "    f1[counter] = f1_score(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    precision, recall, _ = precision_recall_curve(birth_ho.birth_class_binary.values, log_reg.predict_proba(birth_ho[model_feat].values)[:,1])\n",
    "    pr_auc[counter] = auc(recall, precision)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    conf_mat = confusion_matrix(birth_ho.birth_class_binary.values, pred)\n",
    "    \n",
    "    print('---Metrics for Fold ', counter+1,'---')\n",
    "    print('Precision: ', prec[counter])\n",
    "    print('Recall: ', recalls[counter])\n",
    "    print('F1: ', f1[counter])\n",
    "    print('AUC: ', pr_auc[counter])\n",
    "    print('Confusion Matrix: ')\n",
    "    print(conf_mat)\n",
    "    print(' ')\n",
    "    \n",
    "    plt.plot(recall,precision, marker='.', label='Logistic')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "    #Adjust counter for next k-fold split\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are the statistics for the recall score of our model:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(f1), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(f1), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(pr_auc), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(pr_auc), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Chosen Model on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline to scale and one-hot encode all features in `birth_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One Hot Encoding of all categorical features\n",
    "\n",
    "num_cols = ['maternal_age', 'BMI']\n",
    "\n",
    "cat_cols = ['smoking_status', 'drinking_frequency', 'six_or_more_drinks_occurrence', \n",
    "                     'birth_order', 'diabetes', 'mental_health', 'drug_use']\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols),\n",
    "                                               ('num', StandardScaler(), num_cols)],\n",
    "                                 remainder='passthrough')\n",
    "\n",
    "birth_test_encoded = preprocessor.fit_transform(birth_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the result back to a DataFrame for easier inspection\n",
    "columns_after_encoding = preprocessor.get_feature_names_out()\n",
    "\n",
    "# Custom function to remove prefixes\n",
    "def remove_prefix(prefix, column_names):\n",
    "    return [name[len(prefix):] if name.startswith(prefix) else name for name in column_names]\n",
    "\n",
    "columns_after_encoding = remove_prefix(\"cat__\", columns_after_encoding)\n",
    "columns_after_encoding = remove_prefix(\"num__\", columns_after_encoding)\n",
    "columns_after_encoding = remove_prefix(\"remainder__\", columns_after_encoding)\n",
    "\n",
    "birth_test_encoded = pd.DataFrame(birth_test_encoded, columns=columns_after_encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "birth_test_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure all data types are integers as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_feat = ['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes',\n",
    "                   'birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other']\n",
    "\n",
    "# Make sure all encoded variables are integer types\n",
    "birth_test_encoded.birth_class_binary = birth_test_encoded.birth_class_binary.astype(int)\n",
    "birth_test_encoded[cat_feat] = birth_test_encoded[cat_feat].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_test_encoded.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Baseline Model on Test Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find proportion of response variable that falls into either category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "normalized_value_counts = birth_test['birth_class'].value_counts(normalize=True)\n",
    "\n",
    "# Display the normalized value counts\n",
    "print(\"Normalized value counts:\")\n",
    "print(normalized_value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth['birth_class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "7568/1203"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `birth_test`, Term births are 86.3% and Preterm births are 13.7%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Initialize empty lists to store evaluation metrics\n",
    "baseline_precision = []\n",
    "baseline_recalls = []\n",
    "baseline_f1_scores = []\n",
    "baseline_pr_aucs = []\n",
    "\n",
    "# Perform 1000 random draws\n",
    "for obs in range(1000):\n",
    "    \n",
    "    # Generate random binomial draws with probability 0.137\n",
    "    draw = np.random.binomial(n=1, p=0.137, size=len(birth_test))\n",
    "  \n",
    "    # Calculate precision score and append to the list\n",
    "    precision_obs = precision_score(birth_test.birth_class_binary, draw)\n",
    "    baseline_precision.append(precision_obs)\n",
    "\n",
    "    # Calculate recall score and append to the list\n",
    "    recall_obs = recall_score(birth_test.birth_class_binary, draw)\n",
    "    baseline_recalls.append(recall_obs)\n",
    "    \n",
    "    # Calculate precision-recall curve and AUC\n",
    "    base_precision, base_recall, _ = precision_recall_curve(birth_test.birth_class_binary, draw)\n",
    "    pr_auc = auc(base_recall, base_precision)\n",
    "    baseline_pr_aucs.append(pr_auc)\n",
    "    \n",
    "    # Calculate F1 score and append to the list\n",
    "    f1 = f1_score(birth_test.birth_class_binary, draw)\n",
    "    baseline_f1_scores.append(f1)\n",
    "\n",
    "plt.plot(base_recall,base_precision, marker='.')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "# Print statistics for recall, F1, and PR AUC scores\n",
    "print(\"Here are some statistics for our baseline:\")\n",
    "\n",
    "print(\"Mean Precision - \" + str(round(np.mean(baseline_precision), 6)))\n",
    "print(\"Median Precision - \" + str(round(np.median(baseline_precision), 6)))\n",
    "\n",
    "print(\"Mean Recall - \" + str(round(np.mean(baseline_recalls), 6)))\n",
    "print(\"Median Recall - \" + str(round(np.median(baseline_recalls), 6)))\n",
    "\n",
    "print(\"Mean F1 Score - \" + str(round(np.mean(baseline_f1_scores), 6)))\n",
    "print(\"Median F1 Score - \" + str(round(np.median(baseline_f1_scores), 6)))\n",
    "\n",
    "print(\"Mean PR AUC - \" + str(round(np.mean(baseline_pr_aucs), 6)))\n",
    "print(\"Median PR AUC - \" + str(round(np.median(baseline_pr_aucs), 6)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Best Model  Logistic Regression with Class Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the model features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These include the following variables:\n",
    "\n",
    "Continuous:\n",
    "- Maternal age\n",
    "- BMI\n",
    "\n",
    "Categorical:\n",
    "- Smoking Status\n",
    "- Drinking frequency\n",
    "- Binge drinking\n",
    "- Drug use\n",
    "- Diabetes\n",
    "- Mental health\n",
    "- Birth order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feat = ['maternal_age', 'BMI']\n",
    "\n",
    "# Add one-hot encoded lifestyle features\n",
    "model_feat.extend(['smoking_status_Non-smoker', \n",
    "                   'smoking_status_Smoker', \n",
    "                   'smoking_status_Unknown', \n",
    "                   'drinking_frequency_2to3Weekly',\n",
    "                   'drinking_frequency_2to4Monthly',\n",
    "                   'drinking_frequency_4orMoreWeekly',\n",
    "                   'drinking_frequency_Monthly',\n",
    "                   'drinking_frequency_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Daily',\n",
    "                   'six_or_more_drinks_occurrence_LessThanMonthly',\n",
    "                   'six_or_more_drinks_occurrence_Monthly',\n",
    "                   'six_or_more_drinks_occurrence_Never',\n",
    "                   'six_or_more_drinks_occurrence_Skipped',\n",
    "                   'six_or_more_drinks_occurrence_Weekly',\n",
    "                   'drug_use_No', \n",
    "                   'drug_use_Yes'])\n",
    "\n",
    "# Add one-hot encoded health features\n",
    "model_feat.extend(['birth_order_1',\n",
    "                   'birth_order_2',\n",
    "                   'birth_order_3',\n",
    "                   'diabetes_None',\n",
    "                   'diabetes_Type1',\n",
    "                   'diabetes_Type2',\n",
    "                   'diabetes_TypeUnknown',\n",
    "                   'mental_health_Anxiety',\n",
    "                   'mental_health_Depression',\n",
    "                   'mental_health_None',\n",
    "                   'mental_health_Other'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Final Model and Print Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # Initialize arrays to store evaluation metrics\n",
    "# prec = np.zeros(num_splits)\n",
    "# recalls = np.zeros(num_splits)\n",
    "# f1 = np.zeros(num_splits)\n",
    "# pr_auc = np.zeros(num_splits)\n",
    "\n",
    "# # Start counter\n",
    "# counter = 0\n",
    "\n",
    "# Loop through each KFold split\n",
    "# for train_index, test_index in kfold.split(birth_train_encoded, birth_train_encoded.birth_class_binary):\n",
    "#     birth_tt = birth_train_encoded.iloc[train_index]\n",
    "#     birth_ho = birth_train_encoded.iloc[test_index]\n",
    "\n",
    "log_reg = LogisticRegression(penalty='l2', class_weight='balanced', max_iter=1000)\n",
    "\n",
    "log_reg.fit(birth_train_encoded[model_feat].values, birth_train_encoded.birth_class_binary.values)\n",
    "\n",
    "pred = log_reg.predict(birth_test_encoded[model_feat].values)\n",
    "\n",
    "precision = precision_score(birth_test_encoded.birth_class_binary.values, pred)\n",
    "recall = recall_score(birth_test_encoded.birth_class_binary.values, pred)\n",
    "f1 = f1_score(birth_test_encoded.birth_class_binary.values, pred)\n",
    "\n",
    "# Calculate precision-recall curve and AUC\n",
    "precisions, recalls, _ = precision_recall_curve(birth_test_encoded.birth_class_binary.values, log_reg.predict_proba(birth_test_encoded[model_feat].values)[:,1])\n",
    "pr_auc = auc(recalls, precisions)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "conf_mat = confusion_matrix(birth_test_encoded.birth_class_binary.values, pred)\n",
    "\n",
    "print('--- Final Model Metrics on Test Data ---')\n",
    "print('Precision: ', precision)\n",
    "print('Recall: ', recall)\n",
    "print('F1: ', f1)\n",
    "print('AUC: ', pr_auc)\n",
    "print('Confusion Matrix: ')\n",
    "print(conf_mat)\n",
    "print(' ')\n",
    "\n",
    "plt.plot(recalls,precisions, marker='.', label='Logistic')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "\n",
    "#Adjust counter for next k-fold split\n",
    "#counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make plot of model PR-AUC vs baseline PR-AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "\n",
    "# Plot baseline\n",
    "plt.plot(base_recall, base_precision, linestyle='dashed', marker='.', label='Baseline Model')\n",
    "\n",
    "# Plot model\n",
    "plt.plot(recalls, precisions, marker='.', label='Final Model')\n",
    "\n",
    "plt.title('Precision-Recall Curves for Baseline and Final Models', fontsize=\"20\")\n",
    "plt.xlabel('Recall', fontsize=\"15\")\n",
    "plt.ylabel('Precision', fontsize=\"15\")\n",
    "plt.legend(fontsize=\"15\", loc =\"upper right\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fairness Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding for fairness metrics because I couldn't get the pipeline to work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_encoded - entire birth dataframe for metrics on entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_encoded = pd.get_dummies(birth, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "# Drop datetime and obj col as it causes errors and is not necessary\n",
    "columns_to_drop = ['condition_start_date', 'birth_class', 'date_of_birth']\n",
    "birth_encoded = birth_encoded.drop(columns=columns_to_drop)\n",
    "\n",
    "\n",
    "birth_encoded = birth_encoded.select_dtypes(exclude=['object'])\n",
    "\n",
    "birth_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_ho encoding (birth holdout set) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_ho_encoded = pd.get_dummies(birth_ho, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class', 'date_of_birth']\n",
    "birth_ho_encoded = birth_ho_encoded.drop(columns=columns_to_drop)\n",
    "birth_ho_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Birth_test_data (Test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "birth_test_data_encoded = pd.get_dummies(birth_test_encoded, columns=['race_person', 'ethnicity_person'], prefix=['race', 'ethnicity'], prefix_sep='_')\n",
    "\n",
    "columns_to_drop = ['condition_start_date', 'birth_class', 'date_of_birth']\n",
    "birth_test_data_encoded = birth_test_data_encoded.drop(columns=columns_to_drop)\n",
    "birth_test_data_encoded.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calculate SPD and Equalized Odds on Training Data Holdout set using loop to access folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install aif360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Assuming total_folds is the number of folds from your previous code\n",
    "total_folds = kfold.get_n_splits()\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "# Iterate over folds\n",
    "for fold in range(total_folds):\n",
    "    \n",
    "    # Use the predictions from the corresponding fold\n",
    "    svc_pred_fold = svc_predictions[fold]\n",
    "\n",
    "    # Iterate over minority groups\n",
    "    for minority_group in race_categories:\n",
    "        \n",
    "        # Create a new BinaryLabelDataset for each minority group within the fold\n",
    "        label_column_name = 'birth_class_binary'\n",
    "        bld = BinaryLabelDataset(\n",
    "            favorable_label=0, unfavorable_label=1,\n",
    "            df=birth_ho_encoded, label_names=[label_column_name],\n",
    "            protected_attribute_names=['race_White'] + race_categories\n",
    "        )\n",
    "\n",
    "        privileged_groups = [{'race_White': 1}]\n",
    "        unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "        # Create an instance of BinaryLabelDatasetMetric\n",
    "        metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "        # Calculate mean difference\n",
    "        mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "        # Assuming you have ground truth labels and predicted labels\n",
    "        cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "        # Calculate equalized odds ratio\n",
    "        equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {np.mean(mean_diffs[minority_group]).round(3)}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {np.mean(equalized_odds_ratios[minority_group]).round(3)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fairness Metrics SPD and Equalized Odds on Test Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import aif360\n",
    "from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "race_categories = ['race_Black or African American', 'race_None of these', 'race_no answer',\n",
    "                   'race_More than one population', 'race_Asian', 'race_Middle Eastern or North African',\n",
    "                   'race_Native Hawaiian or Other Pacific Islander']\n",
    "\n",
    "# Initialize arrays to store fairness metrics across folds for each minority group\n",
    "mean_diffs = {group: [] for group in race_categories}\n",
    "equalized_odds_ratios = {group: [] for group in race_categories}\n",
    "\n",
    "\n",
    "# Iterate over minority groups\n",
    "for minority_group in race_categories:\n",
    "\n",
    "    # Create a new BinaryLabelDataset for each minority group\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_test_data_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White'] + race_categories\n",
    "    )\n",
    "\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld = BinaryLabelDatasetMetric(bld, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate mean difference\n",
    "    mean_diffs[minority_group].append(metric_bld.mean_difference())\n",
    "\n",
    "    # Assuming you have ground truth labels and predicted labels\n",
    "    cm = ClassificationMetric(bld, bld, unprivileged_groups=unprivileged_groups, privileged_groups=privileged_groups)\n",
    "\n",
    "    # Calculate equalized odds ratio\n",
    "    equalized_odds_ratios[minority_group].append(cm.average_odds_difference())\n",
    "\n",
    "# Print the mean fairness metrics for each minority group across all folds\n",
    "for minority_group in race_categories:\n",
    "    print(f\" Mean Difference ({minority_group}): {mean_diffs[minority_group]}\")\n",
    "    print(f\" Equalized Odds Ratio ({minority_group}): {equalized_odds_ratios[minority_group]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ground truth SPD calculation (this applies to entire dataset to see what our actual disparities are); no pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "\n",
    "for minority_group in race_categories:\n",
    "    \n",
    "\n",
    "    # Create a BinaryLabelDataset for ground truth labels\n",
    "    label_column_name = 'birth_class_binary'\n",
    "    bld_ground_truth = BinaryLabelDataset(\n",
    "        favorable_label=0, unfavorable_label=1,\n",
    "        df=birth_encoded, label_names=[label_column_name],\n",
    "        protected_attribute_names=['race_White', minority_group]\n",
    "    )\n",
    "\n",
    "    # Set privileged and unprivileged groups\n",
    "    privileged_groups = [{'race_White': 1}]\n",
    "    unprivileged_groups = [{'race_White': 0, minority_group: 1}]\n",
    "\n",
    "    # Create an instance of BinaryLabelDatasetMetric\n",
    "    metric_bld_ground_truth = BinaryLabelDatasetMetric(bld_ground_truth, privileged_groups=privileged_groups, unprivileged_groups=unprivileged_groups)\n",
    "\n",
    "    # Calculate SPD for ground truth labels\n",
    "    spd_ground_truth = metric_bld_ground_truth.statistical_parity_difference()\n",
    "              \n",
    "\n",
    "    print(f\" Statistical Parity Difference (Ground Truth):\", minority_group, spd_ground_truth)\n",
    "   \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "436.833px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
